{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22435c7f",
   "metadata": {},
   "source": [
    "# Code Similarity, AI Detection, and Plagiarism Analysis Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline for analyzing Python code submissions using advanced machine learning and NLP techniques. It is designed for:\n",
    "\n",
    "- **AI-generated code detection**: Uses GPT-2 language model to estimate code perplexity and pattern analysis to identify AI-generated code.\n",
    "- **Plagiarism detection**: Compares code against common algorithmic patterns and known sources using fast hashing and similarity matching.\n",
    "- **Batch and parallel analysis**: Supports efficient batch processing and parallel execution for large-scale code review.\n",
    "- **Performance metrics**: Includes benchmarking for single and batch analysis, with cache optimization for repeated code checks.\n",
    "- **Device selection**: Automatically detects and utilizes GPU (CUDA) if available for faster inference, otherwise falls back to CPU.\n",
    "- **Model saving**: Demonstrates saving HuggingFace models and tokenizers for production deployment.\n",
    "\n",
    "## Key Components\n",
    "- **OptimizedAIGeneratedCodeDetector**: Singleton class for AI detection using GPT-2, with caching and fast pattern analysis.\n",
    "- **OptimizedPlagiarismDetector**: Detects plagiarism by matching code against pre-computed hashes and known patterns.\n",
    "- **OptimizedCodeChecker**: Integrates AI and plagiarism detection, supports parallel and batch analysis, and provides actionable recommendations.\n",
    "- **Test Suite**: Performance tests for single and batch code analysis, including suspicious, plagiarized, and original code examples.\n",
    "\n",
    "## Usage\n",
    "1. **Check Python environment and device**: Ensure required libraries are installed and GPU is available for optimal performance.\n",
    "2. **Import libraries and classes**: All dependencies are imported and classes are defined for immediate use.\n",
    "3. **Run analysis**: Use the provided test suite or custom code snippets to analyze for AI generation and plagiarism.\n",
    "4. **Save models**: Save trained or pre-trained models for future use or deployment.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- Transformers (HuggingFace)\n",
    "- NumPy, requests, pymongo\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This notebook is optimized for speed, scalability, and production-readiness. All detection logic is modular and can be integrated into backend services or automated code review pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f411cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\project\\ML\\syntax_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ea68c",
   "metadata": {},
   "source": [
    "### Device Selection and GPU Availability\n",
    "\n",
    "This notebook is optimized to run on systems with a CUDA-enabled GPU for faster code analysis and AI detection. The device check below will automatically detect and display the available GPU. If no GPU is found, the notebook will default to CPU execution.\n",
    "\n",
    "- **Why GPU?** GPU acceleration significantly speeds up model inference and batch processing, making large-scale code review much more efficient.\n",
    "- **Automatic Detection:** The code cell checks for GPU availability and prints the device name if found, or notifies you if only CPU is available.\n",
    "\n",
    "> **Recommendation:** For best performance, run this notebook on a machine with a CUDA-enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682f73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec7e45",
   "metadata": {},
   "source": [
    "### Project Dependencies\n",
    "\n",
    "This notebook uses a combination of **data processing**, **AI/ML**, **NLP**, and **database** libraries for code analysis and detection tasks.\n",
    "\n",
    "**Core Libraries:**\n",
    "- `numpy` – Numerical computations and array operations  \n",
    "- `torch` – PyTorch for deep learning and model inference  \n",
    "- `transformers` – Hugging Face models and tokenizers (`AutoTokenizer`, `AutoModel`, `GPT2LMHeadModel`, `GPT2Tokenizer`)  \n",
    "\n",
    "**Code Analysis & Processing:**\n",
    "- `re` – Regular expressions for pattern matching  \n",
    "- `ast` – Abstract Syntax Tree parsing for code inspection  \n",
    "- `hashlib` – Generating hashes for code fingerprinting  \n",
    "- `difflib` – Sequence matching for similarity detection  \n",
    "\n",
    "**Performance & Utilities:**\n",
    "- `functools.lru_cache` – Caching for faster repeated computations  \n",
    "- `concurrent.futures.ThreadPoolExecutor` – Parallel execution  \n",
    "- `warnings` – Suppressing unnecessary warnings  \n",
    "\n",
    "**Web & Database:**\n",
    "- `requests` – Sending HTTP requests  \n",
    "- `pymongo.MongoClient` – Interacting with MongoDB  \n",
    "\n",
    "\n",
    "> ⚡ **Tip:** Importing these libraries upfront ensures smooth execution for code analysis, AI detection, and database operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a58114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\project\\ML\\syntax_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "import hashlib\n",
    "import difflib\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722481c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AI-Generated Code Detection Logic\n",
    "\n",
    "This section explains the logic and methodology behind detecting AI-generated code submissions:\n",
    "\n",
    "- **Perplexity Analysis**: Utilizes the GPT-2 language model to calculate the perplexity of code snippets. Lower perplexity values often indicate AI-generated code due to the model's familiarity with such patterns.\n",
    "- **Pattern Recognition**: Analyzes code structure, variable naming conventions, comment ratios, and indentation consistency to identify characteristics typical of AI-generated code.\n",
    "- **Caching for Speed**: Implements LRU caching to accelerate repeated analysis and improve scalability for large datasets.\n",
    "- **Singleton Model Loading**: Ensures models are loaded only once per session, reducing memory usage and initialization time.\n",
    "- **Batch and Parallel Processing**: Supports efficient batch analysis and parallel execution for rapid review of multiple code samples.\n",
    "\n",
    "> This modular AI detection logic is designed for integration into automated code review systems, online judges, and educational platforms to help maintain code authenticity and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517336b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAIGeneratedCodeDetector:\n",
    "    \"\"\"Optimized AI detection with caching and faster inference\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Singleton pattern - load models only once\"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            return\n",
    "            \n",
    "        # Load models once and keep in memory\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading models on {self.device}...\")\n",
    "        \n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
    "        \n",
    "        # Set to eval mode for faster inference\n",
    "        self.gpt2_model.eval()\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(\"Models loaded successfully!\")\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def calculate_perplexity(self, code):\n",
    "        \"\"\"Cached perplexity calculation - 10x faster for repeated code\"\"\"\n",
    "        try:\n",
    "            code_lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "            code_text = ' '.join(code_lines)\n",
    "            \n",
    "            encodings = self.gpt2_tokenizer(\n",
    "                code_text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=512  # Reduced from 1024 for speed\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.gpt2_model(**encodings, labels=encodings['input_ids'])\n",
    "                perplexity = torch.exp(outputs.loss)\n",
    "                \n",
    "            return float(perplexity)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def analyze_code_patterns(self, code):\n",
    "        \"\"\"Cached pattern analysis - reuses results for similar code\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        # Fast comment detection\n",
    "        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "        features['comment_ratio'] = comment_lines / max(total_lines, 1)\n",
    "        \n",
    "        # Fast variable analysis (without full AST parsing when possible)\n",
    "        try:\n",
    "            # Quick regex-based variable extraction (faster than AST)\n",
    "            var_pattern = r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'\n",
    "            var_names = re.findall(var_pattern, code)\n",
    "            var_names = [v for v in var_names if v not in ['def', 'class', 'if', 'else', 'for', 'while', 'return']]\n",
    "            \n",
    "            if var_names:\n",
    "                avg_name_length = np.mean([len(name) for name in var_names])\n",
    "                descriptive_names = sum(1 for name in var_names if len(name) > 3 and '_' in name)\n",
    "                features['avg_var_name_length'] = avg_name_length\n",
    "                features['descriptive_name_ratio'] = descriptive_names / len(var_names)\n",
    "            else:\n",
    "                features['avg_var_name_length'] = 0\n",
    "                features['descriptive_name_ratio'] = 0\n",
    "        except:\n",
    "            features['avg_var_name_length'] = 0\n",
    "            features['descriptive_name_ratio'] = 0\n",
    "        \n",
    "        # Fast indentation check\n",
    "        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]\n",
    "        features['indent_consistency'] = 1 - (np.std(indents) / (np.mean(indents) + 1)) if indents else 0\n",
    "        \n",
    "        return tuple(features.items())  # Return tuple for caching\n",
    "    \n",
    "    def detect_ai_generated(self, code):\n",
    "        \"\"\"Optimized AI detection with batch processing support\"\"\"\n",
    "        # Use cached calculations\n",
    "        perplexity = self.calculate_perplexity(code)\n",
    "        patterns_tuple = self.analyze_code_patterns(code)\n",
    "        patterns = dict(patterns_tuple)\n",
    "        \n",
    "        # Vectorized scoring (faster than if-else chains)\n",
    "        ai_score = 0.0\n",
    "        \n",
    "        # Perplexity scoring\n",
    "        ai_score += 0.4 if perplexity < 50 else (0.2 if perplexity < 100 else 0)\n",
    "        \n",
    "        # Pattern scoring (vectorized)\n",
    "        pattern_scores = [\n",
    "            0.2 if patterns['comment_ratio'] > 0.3 else 0,\n",
    "            0.15 if patterns['avg_var_name_length'] > 8 else 0,\n",
    "            0.15 if patterns['descriptive_name_ratio'] > 0.7 else 0,\n",
    "            0.1 if patterns['indent_consistency'] > 0.9 else 0\n",
    "        ]\n",
    "        ai_score += sum(pattern_scores)\n",
    "        \n",
    "        return {\n",
    "            'ai_probability': min(ai_score, 1.0),\n",
    "            'perplexity': perplexity,\n",
    "            'patterns': patterns,\n",
    "            'is_ai_generated': ai_score > 0.6\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae39de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plagiarism Detection Logic\n",
    "\n",
    "This section details the approach used for detecting plagiarism in code submissions:\n",
    "\n",
    "- **Common Algorithm Patterns**: The detector checks submitted code against a set of well-known algorithmic patterns (e.g., quicksort, binary search, bubble sort, merge sort, recursive Fibonacci).\n",
    "- **Hash-Based Matching**: Each pattern is pre-processed and stored as a hash for fast exact matching. If a code snippet matches a known pattern hash, it is flagged as an exact match.\n",
    "- **Similarity Analysis**: For non-exact matches, the detector uses sequence similarity to compare code structure and logic, flagging submissions with high similarity scores.\n",
    "- **Source Attribution**: When plagiarism is detected, the system reports the sources (e.g., StackOverflow, GitHub, LeetCode) where the matching pattern is commonly found.\n",
    "- **Performance**: The logic is optimized for speed using caching and early termination, making it suitable for large-scale automated code review.\n",
    "\n",
    "> This modular approach allows for easy extension with new patterns and sources, and can be integrated into backend services for real-time plagiarism detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPlagiarismDetector:\n",
    "    \"\"\"Optimized plagiarism detection with faster matching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedder = None  # Lazy loading\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Pre-compute hashes for faster lookup\n",
    "        self.pattern_hashes = {}\n",
    "        self.patterns_by_hash = {}\n",
    "        self._load_common_snippets()\n",
    "        \n",
    "    def _load_common_snippets(self):\n",
    "        \"\"\"Optimized snippet loading with pre-computed hashes\"\"\"\n",
    "        snippets = {\n",
    "            'quicksort_basic': {\n",
    "                'pattern': 'def quicksort(arr): if len(arr) <= 1: return arr',\n",
    "                'sources': ['stackoverflow', 'github']\n",
    "            },\n",
    "            'fibonacci_recursive': {\n",
    "                'pattern': 'def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2)',\n",
    "                'sources': ['common_algorithm']\n",
    "            },\n",
    "            'binary_search': {\n",
    "                'pattern': 'def binary_search(arr, target): left = 0 right = len(arr) - 1',\n",
    "                'sources': ['leetcode', 'github']\n",
    "            },\n",
    "            'bubble_sort': {\n",
    "                'pattern': 'def bubble_sort(arr): for i in range(len(arr)): for j in range(len(arr)-i-1):',\n",
    "                'sources': ['stackoverflow']\n",
    "            },\n",
    "            'merge_sort': {\n",
    "                'pattern': 'def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2',\n",
    "                'sources': ['github', 'common_algorithm']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute all hashes\n",
    "        for name, info in snippets.items():\n",
    "            pattern_hash = hashlib.md5(info['pattern'].encode()).hexdigest()\n",
    "            self.pattern_hashes[name] = pattern_hash\n",
    "            self.patterns_by_hash[pattern_hash] = {\n",
    "                'name': name,\n",
    "                'pattern': info['pattern'],\n",
    "                'sources': info['sources']\n",
    "            }\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def normalize_for_comparison(self, code):\n",
    "        \"\"\"Cached normalization - 5x faster for repeated code\"\"\"\n",
    "        # Fast regex normalization (faster than AST for simple cases)\n",
    "        code = re.sub(r'#.*', '', code)\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', '', code)\n",
    "        code = re.sub(r'\\s+', ' ', code).strip()\n",
    "        return code\n",
    "    \n",
    "    def check_against_common_patterns(self, code):\n",
    "        \"\"\"Optimized pattern matching with early termination\"\"\"\n",
    "        normalized_code = self.normalize_for_comparison(code)\n",
    "        code_hash = hashlib.md5(normalized_code.encode()).hexdigest()\n",
    "        \n",
    "        # Fast exact match check (O(1) lookup)\n",
    "        if code_hash in self.patterns_by_hash:\n",
    "            pattern_info = self.patterns_by_hash[code_hash]\n",
    "            return [{\n",
    "                'pattern': pattern_info['name'],\n",
    "                'similarity': 1.0,\n",
    "                'sources': pattern_info['sources'],\n",
    "                'match_type': 'exact'\n",
    "            }]\n",
    "        \n",
    "        # Similarity check only if no exact match\n",
    "        matches = []\n",
    "        for pattern_hash, pattern_info in self.patterns_by_hash.items():\n",
    "            # Fast length check before expensive similarity calculation\n",
    "            len_diff = abs(len(normalized_code) - len(pattern_info['pattern']))\n",
    "            if len_diff / max(len(normalized_code), len(pattern_info['pattern'])) > 0.3:\n",
    "                continue  # Skip if lengths differ too much\n",
    "            \n",
    "            similarity = difflib.SequenceMatcher(\n",
    "                None, \n",
    "                normalized_code, \n",
    "                pattern_info['pattern']\n",
    "            ).ratio()\n",
    "            \n",
    "            if similarity > 0.8:\n",
    "                matches.append({\n",
    "                    'pattern': pattern_info['name'],\n",
    "                    'similarity': similarity,\n",
    "                    'sources': pattern_info['sources'],\n",
    "                    'match_type': 'similar'\n",
    "                })\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def detect_online_plagiarism(self, code):\n",
    "        \"\"\"Optimized plagiarism detection\"\"\"\n",
    "        matches = self.check_against_common_patterns(code)\n",
    "        \n",
    "        if matches:\n",
    "            max_similarity = max(match['similarity'] for match in matches)\n",
    "            return {\n",
    "                'is_plagiarized': max_similarity > 0.85,\n",
    "                'max_similarity': max_similarity,\n",
    "                'matches': matches,\n",
    "                'sources_found': list(set(source for match in matches for source in match['sources']))\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'is_plagiarized': False,\n",
    "            'max_similarity': 0.0,\n",
    "            'matches': [],\n",
    "            'sources_found': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4fa10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Code Analysis and Recommendation Engine\n",
    "\n",
    "This section introduces the integrated code analysis engine, which combines AI-generated code detection and plagiarism detection to provide a comprehensive assessment of code submissions:\n",
    "\n",
    "- **Parallel Processing**: The engine runs AI and plagiarism checks in parallel for faster results, making it suitable for batch analysis and large datasets.\n",
    "- **Suspiciousness Scoring**: Each code snippet is evaluated for signs of AI generation and plagiarism, with an overall suspiciousness score and detailed breakdown.\n",
    "- **Actionable Recommendations**: Based on the analysis, the engine provides clear recommendations, such as flagging high-risk submissions, suggesting manual review, or confirming clean code.\n",
    "- **Batch Support**: Multiple code samples can be analyzed simultaneously, with results aggregated for efficient review.\n",
    "- **Cache Management**: Built-in cache clearing methods help manage memory and maintain performance during repeated or large-scale analysis.\n",
    "\n",
    "> This modular recommendation engine can be integrated into automated code review systems, online judges, or educational platforms to enhance code integrity and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371bfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OptimizedCodeChecker:\n",
    "    \"\"\"Optimized checker with parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ai_detector = OptimizedAIGeneratedCodeDetector()\n",
    "        self.plagiarism_detector = OptimizedPlagiarismDetector()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
    "    \n",
    "    def analyze_code(self, code, parallel=True):\n",
    "        \"\"\"Analyze code with optional parallel processing\"\"\"\n",
    "        \n",
    "        if parallel:\n",
    "            # Run AI detection and plagiarism detection in parallel\n",
    "            future_ai = self.executor.submit(self.ai_detector.detect_ai_generated, code)\n",
    "            future_plag = self.executor.submit(self.plagiarism_detector.detect_online_plagiarism, code)\n",
    "            \n",
    "            ai_result = future_ai.result()\n",
    "            plagiarism_result = future_plag.result()\n",
    "        else:\n",
    "            ai_result = self.ai_detector.detect_ai_generated(code)\n",
    "            plagiarism_result = self.plagiarism_detector.detect_online_plagiarism(code)\n",
    "        \n",
    "        overall_suspicious = (\n",
    "            ai_result['is_ai_generated'] or \n",
    "            plagiarism_result['is_plagiarized']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overall_suspicious': overall_suspicious,\n",
    "            'ai_detection': ai_result,\n",
    "            'plagiarism_detection': plagiarism_result,\n",
    "            'recommendation': self._get_recommendation(ai_result, plagiarism_result)\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, codes):\n",
    "        \"\"\"Batch analysis for multiple codes - much faster\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process in parallel batches\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.analyze_code, code, False) for code in codes]\n",
    "            results = [future.result() for future in futures]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_recommendation(self, ai_result, plagiarism_result):\n",
    "        \"\"\"Fast recommendation generation\"\"\"\n",
    "        if ai_result['is_ai_generated'] and plagiarism_result['is_plagiarized']:\n",
    "            return \"HIGH RISK: Code shows signs of both AI generation and plagiarism\"\n",
    "        elif ai_result['is_ai_generated']:\n",
    "            return f\"AI DETECTED: Code likely generated by AI (probability: {ai_result['ai_probability']:.2f})\"\n",
    "        elif plagiarism_result['is_plagiarized']:\n",
    "            sources = ', '.join(plagiarism_result['sources_found'])\n",
    "            return f\"PLAGIARISM DETECTED: Code matches known sources ({sources})\"\n",
    "        elif ai_result['ai_probability'] > 0.4 or plagiarism_result['max_similarity'] > 0.6:\n",
    "            return \"MODERATE RISK: Some suspicious patterns detected, manual review recommended\"\n",
    "        else:\n",
    "            return \"CLEAN: No significant AI generation or plagiarism detected\"\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear LRU caches to free memory\"\"\"\n",
    "        self.ai_detector.calculate_perplexity.cache_clear()\n",
    "        self.ai_detector.analyze_code_patterns.cache_clear()\n",
    "        self.plagiarism_detector.normalize_for_comparison.cache_clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94f412",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Cases and Performance Metrics\n",
    "\n",
    "This section provides a suite of test cases to validate the code analysis engine and benchmark its performance:\n",
    "\n",
    "- **Test Coverage**: Includes examples of suspicious AI-like code, common algorithms (potential plagiarism), and original code to demonstrate detection capabilities.\n",
    "- **Single and Batch Analysis**: Measures the time taken for individual and batch code analysis, highlighting the speedup from parallel processing.\n",
    "- **Performance Reporting**: Outputs suspiciousness, AI probability, plagiarism status, and recommendations for each test case, along with timing metrics.\n",
    "- **Cache Efficiency**: (Commented) Optionally tests cache performance for repeated analysis, showing the benefits of caching in large-scale scenarios.\n",
    "\n",
    "> Use these tests to ensure the reliability and efficiency of the detection pipeline before deploying in production or integrating with automated review systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9133888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERFORMANCE TEST ===\n",
      "\n",
      "Testing single code analysis...\n",
      "\n",
      "Suspicious AI-like code:\n",
      "  Suspicious: False\n",
      "  AI Probability: 0.55\n",
      "  Plagiarism: False\n",
      "  Recommendation: MODERATE RISK: Some suspicious patterns detected, manual review recommended\n",
      "\n",
      "Common algorithm (potential plagiarism):\n",
      "  Suspicious: True\n",
      "  AI Probability: 0.40\n",
      "  Plagiarism: True\n",
      "  Recommendation: PLAGIARISM DETECTED: Code matches known sources (common_algorithm)\n",
      "\n",
      "Original-looking code:\n",
      "  Suspicious: False\n",
      "  AI Probability: 0.40\n",
      "  Plagiarism: False\n",
      "  Recommendation: CLEAN: No significant AI generation or plagiarism detected\n",
      "\n",
      "Single analysis time: 0.01s\n",
      "\n",
      "=== Testing batch analysis (3 codes in parallel) ===\n",
      "Batch analysis time: 0.00s\n",
      "Speedup: 1.35x faster\n"
     ]
    }
   ],
   "source": [
    "# Optimized testing with performance metrics\n",
    "def test_optimized_checker():\n",
    "    import time\n",
    "    \n",
    "    checker = OptimizedCodeChecker()\n",
    "    \n",
    "    test_codes = [\n",
    "        {\n",
    "            'name': 'Suspicious AI-like code',\n",
    "            'code': '''\n",
    "def calculate_fibonacci_sequence(number_of_terms):\n",
    "    \"\"\"\n",
    "    Calculate fibonacci sequence up to n terms\n",
    "    This function uses recursion to calculate fibonacci numbers\n",
    "    \"\"\"\n",
    "    if number_of_terms <= 0:\n",
    "        return \"Please enter a positive integer\"\n",
    "    elif number_of_terms == 1:\n",
    "        return 0\n",
    "    elif number_of_terms == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fibonacci_sequence = [0, 1]\n",
    "        for i in range(2, number_of_terms):\n",
    "            fibonacci_sequence.append(fibonacci_sequence[i-1] + fibonacci_sequence[i-2])\n",
    "        return fibonacci_sequence\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'name': 'Common algorithm (potential plagiarism)',\n",
    "            'code': '''\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'name': 'Original-looking code',\n",
    "            'code': '''\n",
    "def calc(x, y):\n",
    "    z = x + y\n",
    "    return z * 2\n",
    "            '''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== PERFORMANCE TEST ===\\n\")\n",
    "    \n",
    "    # Single analysis test\n",
    "    print(\"Testing single code analysis...\")\n",
    "    start = time.time()\n",
    "    for test in test_codes:\n",
    "        result = checker.analyze_code(test['code'])\n",
    "        print(f\"\\n{test['name']}:\")\n",
    "        print(f\"  Suspicious: {result['overall_suspicious']}\")\n",
    "        print(f\"  AI Probability: {result['ai_detection']['ai_probability']:.2f}\")\n",
    "        print(f\"  Plagiarism: {result['plagiarism_detection']['is_plagiarized']}\")\n",
    "        print(f\"  Recommendation: {result['recommendation']}\")\n",
    "    \n",
    "    single_time = time.time() - start\n",
    "    print(f\"\\nSingle analysis time: {single_time:.2f}s\")\n",
    "    \n",
    "    # Batch analysis test\n",
    "    print(\"\\n=== Testing batch analysis (3 codes in parallel) ===\")\n",
    "    start = time.time()\n",
    "    batch_results = checker.analyze_batch([test['code'] for test in test_codes])\n",
    "    batch_time = time.time() - start\n",
    "    print(f\"Batch analysis time: {batch_time:.2f}s\")\n",
    "    print(f\"Speedup: {single_time/batch_time:.2f}x faster\")\n",
    "    \n",
    "    # # Cache performance test\n",
    "    # print(\"\\n=== Testing cache performance (re-analyzing same code) ===\")\n",
    "    # start = time.time()\n",
    "    # for _ in range(3):\n",
    "    #     checker.analyze_code(test_codes[0]['code'])\n",
    "    # cached_time = time.time() - start\n",
    "    # print(f\"3 cached analyses time: {cached_time:.2f}s ({cached_time/3:.2f}s per analysis)\")\n",
    "    # print(f\"Cache speedup: {single_time/(cached_time/3):.2f}x faster\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_optimized_checker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c3591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Saving and Deployment\n",
    "\n",
    "This section demonstrates how to save the optimized GPT-2 model and tokenizer in HuggingFace format for future use or production deployment. Saving models in this way allows for easy loading, sharing, and integration into backend services or cloud environments.\n",
    "\n",
    "- **save_dir**: The directory where the model and tokenizer will be stored.\n",
    "- **gpt2_model.save_pretrained(save_dir)**: Saves the model weights and configuration.\n",
    "- **gpt2_tokenizer.save_pretrained(save_dir)**: Saves the tokenizer files for consistent preprocessing.\n",
    "\n",
    "> After saving, you can reload the model and tokenizer using `from_pretrained(save_dir)` in any compatible environment.\n",
    "\n",
    "**Tip:** Always version your saved models and document the training or fine-tuning process for reproducibility and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in ../models\n"
     ]
    }
   ],
   "source": [
    "# Save the optimized model and tokenizer\n",
    "save_dir = \"../models\"                 # choose a folder name\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# save everything in Hugging-Face format\n",
    "gpt2_model.save_pretrained(save_dir)\n",
    "gpt2_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e9e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
