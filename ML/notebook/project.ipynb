{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22435c7f",
   "metadata": {},
   "source": [
    "# Code Similarity, AI Detection, and Plagiarism Analysis Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline for analyzing Python code submissions using advanced machine learning and NLP techniques. It is designed for:\n",
    "\n",
    "- **AI-generated code detection**: Uses GPT-2 language model to estimate code perplexity and pattern analysis to identify AI-generated code.\n",
    "- **Plagiarism detection**: Compares code against common algorithmic patterns and known sources using fast hashing and similarity matching.\n",
    "- **Batch and parallel analysis**: Supports efficient batch processing and parallel execution for large-scale code review.\n",
    "- **Performance metrics**: Includes benchmarking for single and batch analysis, with cache optimization for repeated code checks.\n",
    "- **Device selection**: Automatically detects and utilizes GPU (CUDA) if available for faster inference, otherwise falls back to CPU.\n",
    "- **Model saving**: Demonstrates saving HuggingFace models and tokenizers for production deployment.\n",
    "\n",
    "## Key Components\n",
    "- **OptimizedAIGeneratedCodeDetector**: Singleton class for AI detection using GPT-2, with caching and fast pattern analysis.\n",
    "- **OptimizedPlagiarismDetector**: Detects plagiarism by matching code against pre-computed hashes and known patterns.\n",
    "- **OptimizedCodeChecker**: Integrates AI and plagiarism detection, supports parallel and batch analysis, and provides actionable recommendations.\n",
    "- **Test Suite**: Performance tests for single and batch code analysis, including suspicious, plagiarized, and original code examples.\n",
    "\n",
    "## Usage\n",
    "1. **Check Python environment and device**: Ensure required libraries are installed and GPU is available for optimal performance.\n",
    "2. **Import libraries and classes**: All dependencies are imported and classes are defined for immediate use.\n",
    "3. **Run analysis**: Use the provided test suite or custom code snippets to analyze for AI generation and plagiarism.\n",
    "4. **Save models**: Save trained or pre-trained models for future use or deployment.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- Transformers (HuggingFace)\n",
    "- NumPy, requests, pymongo\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This notebook is optimized for speed, scalability, and production-readiness. All detection logic is modular and can be integrated into backend services or automated code review pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f411cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\project\\ML\\syntax_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ea68c",
   "metadata": {},
   "source": [
    "### Device Selection and GPU Availability\n",
    "\n",
    "This notebook is optimized to run on systems with a CUDA-enabled GPU for faster code analysis and AI detection. The device check below will automatically detect and display the available GPU. If no GPU is found, the notebook will default to CPU execution.\n",
    "\n",
    "- **Why GPU?** GPU acceleration significantly speeds up model inference and batch processing, making large-scale code review much more efficient.\n",
    "- **Automatic Detection:** The code cell checks for GPU availability and prints the device name if found, or notifies you if only CPU is available.\n",
    "\n",
    "> **Recommendation:** For best performance, run this notebook on a machine with a CUDA-enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682f73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec7e45",
   "metadata": {},
   "source": [
    "### Project Dependencies\n",
    "\n",
    "This notebook uses a combination of **data processing**, **AI/ML**, **NLP**, and **database** libraries for code analysis and detection tasks.\n",
    "\n",
    "**Core Libraries:**\n",
    "- `numpy` â€“ Numerical computations and array operations  \n",
    "- `torch` â€“ PyTorch for deep learning and model inference  \n",
    "- `transformers` â€“ Hugging Face models and tokenizers (`AutoTokenizer`, `AutoModel`, `GPT2LMHeadModel`, `GPT2Tokenizer`)  \n",
    "\n",
    "**Code Analysis & Processing:**\n",
    "- `re` â€“ Regular expressions for pattern matching  \n",
    "- `ast` â€“ Abstract Syntax Tree parsing for code inspection  \n",
    "- `hashlib` â€“ Generating hashes for code fingerprinting  \n",
    "- `difflib` â€“ Sequence matching for similarity detection  \n",
    "\n",
    "**Performance & Utilities:**\n",
    "- `functools.lru_cache` â€“ Caching for faster repeated computations  \n",
    "- `concurrent.futures.ThreadPoolExecutor` â€“ Parallel execution  \n",
    "- `warnings` â€“ Suppressing unnecessary warnings  \n",
    "\n",
    "**Web & Database:**\n",
    "- `requests` â€“ Sending HTTP requests  \n",
    "- `pymongo.MongoClient` â€“ Interacting with MongoDB  \n",
    "\n",
    "\n",
    "> âš¡ **Tip:** Importing these libraries upfront ensures smooth execution for code analysis, AI detection, and database operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a58114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\project\\ML\\syntax_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "import hashlib\n",
    "import difflib\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722481c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AI-Generated Code Detection Logic\n",
    "\n",
    "This section explains the logic and methodology behind detecting AI-generated code submissions:\n",
    "\n",
    "- **Perplexity Analysis**: Utilizes the GPT-2 language model to calculate the perplexity of code snippets. Lower perplexity values often indicate AI-generated code due to the model's familiarity with such patterns.\n",
    "- **Pattern Recognition**: Analyzes code structure, variable naming conventions, comment ratios, and indentation consistency to identify characteristics typical of AI-generated code.\n",
    "- **Caching for Speed**: Implements LRU caching to accelerate repeated analysis and improve scalability for large datasets.\n",
    "- **Singleton Model Loading**: Ensures models are loaded only once per session, reducing memory usage and initialization time.\n",
    "- **Batch and Parallel Processing**: Supports efficient batch analysis and parallel execution for rapid review of multiple code samples.\n",
    "\n",
    "> This modular AI detection logic is designed for integration into automated code review systems, online judges, and educational platforms to help maintain code authenticity and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517336b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAIGeneratedCodeDetector:\n",
    "    \"\"\"\n",
    "    Enhanced AI detection with AST features + Perplexity\n",
    "    No training required - uses pre-trained GPT-2 + rule-based AST parsing\n",
    "    \"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Singleton pattern - load models only once\"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            return\n",
    "            \n",
    "        # Load GPT-2 pre-trained model (no training needed)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading GPT-2 model on {self.device}...\")\n",
    "        \n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
    "        \n",
    "        # Set to eval mode for faster inference\n",
    "        self.gpt2_model.eval()\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(\"âœ“ GPT-2 model loaded successfully (pre-trained, no training required)!\")\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def calculate_perplexity(self, code):\n",
    "        \"\"\"\n",
    "        Cached perplexity calculation using pre-trained GPT-2\n",
    "        Low perplexity = AI-likely (predictable), High = Human-likely (variable)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            code_lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "            code_text = ' '.join(code_lines)\n",
    "            \n",
    "            encodings = self.gpt2_tokenizer(\n",
    "                code_text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.gpt2_model(**encodings, labels=encodings['input_ids'])\n",
    "                perplexity = torch.exp(outputs.loss)\n",
    "                \n",
    "            return float(perplexity)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def extract_ast_features(self, code):\n",
    "        \"\"\"\n",
    "        Extract structural features using AST (rule-based, no training)\n",
    "        AI code shows different structural patterns than human code\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # 1. Node Type Distribution\n",
    "            node_types = [type(node).__name__ for node in ast.walk(tree)]\n",
    "            total_nodes = len(node_types)\n",
    "            features['total_nodes'] = total_nodes\n",
    "            features['unique_node_ratio'] = len(set(node_types)) / max(total_nodes, 1)\n",
    "            \n",
    "            # 2. Depth Analysis (AI tends to have uniform depth)\n",
    "            def get_depths(node, depth=0):\n",
    "                depths = [depth]\n",
    "                for child in ast.iter_child_nodes(node):\n",
    "                    depths.extend(get_depths(child, depth + 1))\n",
    "                return depths\n",
    "            \n",
    "            depths = get_depths(tree)\n",
    "            features['max_depth'] = max(depths) if depths else 0\n",
    "            features['avg_depth'] = np.mean(depths) if depths else 0\n",
    "            features['depth_variance'] = np.var(depths) if len(depths) > 1 else 0\n",
    "            \n",
    "            # 3. Control Flow Complexity\n",
    "            num_functions = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.FunctionDef))\n",
    "            num_conditions = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.If))\n",
    "            num_loops = sum(1 for _ in ast.walk(tree) if isinstance(_, (ast.For, ast.While)))\n",
    "            num_try = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.Try))\n",
    "            \n",
    "            features['num_functions'] = num_functions\n",
    "            features['num_conditions'] = num_conditions\n",
    "            features['num_loops'] = num_loops\n",
    "            features['num_try_except'] = num_try\n",
    "            features['cyclomatic_complexity'] = 1 + num_conditions + num_loops\n",
    "            \n",
    "            # 4. AI-Specific Patterns\n",
    "            # AI code tends to be \"too perfect\" - balanced structure, low variance\n",
    "            features['is_perfectly_balanced'] = (\n",
    "                features['depth_variance'] < 0.8 and \n",
    "                features['cyclomatic_complexity'] < 4 and\n",
    "                features['max_depth'] < 6\n",
    "            )\n",
    "            \n",
    "            # AI code rarely has try-except (error handling)\n",
    "            features['has_error_handling'] = num_try > 0\n",
    "            \n",
    "            # 5. Complexity Ratio (AI avoids very simple or very complex)\n",
    "            if total_nodes > 0:\n",
    "                features['complexity_ratio'] = features['cyclomatic_complexity'] / total_nodes\n",
    "            else:\n",
    "                features['complexity_ratio'] = 0\n",
    "            \n",
    "            # 6. Pattern Repetition (AI often repeats similar structures)\n",
    "            node_type_counts = {}\n",
    "            for node_type in node_types:\n",
    "                node_type_counts[node_type] = node_type_counts.get(node_type, 0) + 1\n",
    "            \n",
    "            if node_type_counts:\n",
    "                max_repetition = max(node_type_counts.values())\n",
    "                features['max_node_repetition'] = max_repetition / total_nodes\n",
    "            else:\n",
    "                features['max_node_repetition'] = 0\n",
    "            \n",
    "            # 7. Parse Success (human code may have minor issues)\n",
    "            features['parse_success'] = True\n",
    "            \n",
    "        except SyntaxError:\n",
    "            # Human code more likely to have syntax quirks\n",
    "            features['parse_success'] = False\n",
    "            features['parse_error'] = True\n",
    "        except Exception as e:\n",
    "            features['parse_success'] = False\n",
    "            features['parse_error'] = True\n",
    "        \n",
    "        return tuple(features.items())  # Return tuple for caching\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def analyze_code_patterns(self, code):\n",
    "        \"\"\"\n",
    "        Fast pattern analysis (regex-based, no training)\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        # 1. Comment Analysis (AI often over-comments or perfectly comments)\n",
    "        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "        docstring_count = len(re.findall(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', code))\n",
    "        features['comment_ratio'] = comment_lines / max(total_lines, 1)\n",
    "        features['has_excessive_comments'] = features['comment_ratio'] > 0.4\n",
    "        features['has_docstrings'] = docstring_count > 0\n",
    "        \n",
    "        # 2. Variable Naming Patterns\n",
    "        try:\n",
    "            var_pattern = r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'\n",
    "            var_names = re.findall(var_pattern, code)\n",
    "            keywords = {'def', 'class', 'if', 'else', 'for', 'while', 'return', 'import', \n",
    "                       'from', 'try', 'except', 'with', 'as', 'in', 'and', 'or', 'not'}\n",
    "            var_names = [v for v in var_names if v not in keywords]\n",
    "            \n",
    "            if var_names:\n",
    "                avg_name_length = np.mean([len(name) for name in var_names])\n",
    "                # AI prefers descriptive names (8+ chars)\n",
    "                descriptive_names = sum(1 for name in var_names if len(name) >= 8)\n",
    "                # AI often uses underscores consistently\n",
    "                underscore_names = sum(1 for name in var_names if '_' in name)\n",
    "                \n",
    "                features['avg_var_name_length'] = avg_name_length\n",
    "                features['descriptive_name_ratio'] = descriptive_names / len(var_names)\n",
    "                features['underscore_ratio'] = underscore_names / len(var_names)\n",
    "            else:\n",
    "                features['avg_var_name_length'] = 0\n",
    "                features['descriptive_name_ratio'] = 0\n",
    "                features['underscore_ratio'] = 0\n",
    "        except:\n",
    "            features['avg_var_name_length'] = 0\n",
    "            features['descriptive_name_ratio'] = 0\n",
    "            features['underscore_ratio'] = 0\n",
    "        \n",
    "        # 3. Indentation Consistency (AI is always perfect)\n",
    "        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]\n",
    "        if indents:\n",
    "            features['indent_consistency'] = 1 - (np.std(indents) / (np.mean(indents) + 1))\n",
    "        else:\n",
    "            features['indent_consistency'] = 0\n",
    "        \n",
    "        # 4. Line Length Uniformity (AI tends to have uniform line lengths)\n",
    "        line_lengths = [len(line) for line in lines if line.strip()]\n",
    "        if line_lengths:\n",
    "            features['line_length_variance'] = np.var(line_lengths)\n",
    "        else:\n",
    "            features['line_length_variance'] = 0\n",
    "        \n",
    "        return tuple(features.items())  # Return tuple for caching\n",
    "    \n",
    "    def detect_ai_generated(self, code):\n",
    "        \"\"\"Fixed scoring with research-backed thresholds\"\"\"\n",
    "        \n",
    "        perplexity = self.calculate_perplexity(code)\n",
    "        ast_features_tuple = self.extract_ast_features(code)\n",
    "        pattern_features_tuple = self.analyze_code_patterns(code)\n",
    "        \n",
    "        ast_features = dict(ast_features_tuple)\n",
    "        pattern_features = dict(pattern_features_tuple)\n",
    "        \n",
    "        ai_score = 0.0\n",
    "        score_breakdown = {}\n",
    "        \n",
    "        # --- FIXED PERPLEXITY SCORING (40% weight, increased) ---\n",
    "        if perplexity < 10:\n",
    "            perplexity_score = 0.40  # VERY AI-like (your case: 5.36)\n",
    "        elif perplexity < 30:\n",
    "            perplexity_score = 0.30  # Strong AI signal\n",
    "        elif perplexity < 60:\n",
    "            perplexity_score = 0.20  # Moderate\n",
    "        elif perplexity < 100:\n",
    "            perplexity_score = 0.10  # Low suspicion\n",
    "        else:\n",
    "            perplexity_score = 0.0   # Human-like (>100)\n",
    "        \n",
    "        ai_score += perplexity_score\n",
    "        score_breakdown['perplexity'] = perplexity_score\n",
    "        \n",
    "        # --- AST FEATURES (35% weight, decreased) ---\n",
    "        ast_score = 0.0\n",
    "        \n",
    "        if not ast_features.get('parse_error', False):\n",
    "            if ast_features.get('parse_success', False):\n",
    "                ast_score += 0.03\n",
    "            if ast_features.get('is_perfectly_balanced', False):\n",
    "                ast_score += 0.15\n",
    "            if ast_features.get('depth_variance', 1.0) < 1.0:\n",
    "                ast_score += 0.08\n",
    "            if ast_features.get('unique_node_ratio', 1.0) < 0.4:\n",
    "                ast_score += 0.08\n",
    "            if not ast_features.get('has_error_handling', False):\n",
    "                ast_score += 0.03\n",
    "        \n",
    "        ai_score += min(ast_score, 0.35)\n",
    "        score_breakdown['ast'] = min(ast_score, 0.35)\n",
    "        \n",
    "        # --- PATTERN SCORING (25% weight, decreased) ---\n",
    "        pattern_score = 0.0\n",
    "        \n",
    "        if pattern_features.get('has_excessive_comments', False):\n",
    "            pattern_score += 0.10\n",
    "        if pattern_features.get('avg_var_name_length', 0) > 10:\n",
    "            pattern_score += 0.06\n",
    "        if pattern_features.get('underscore_ratio', 0) > 0.7:\n",
    "            pattern_score += 0.04\n",
    "        if pattern_features.get('indent_consistency', 0) > 0.95:\n",
    "            pattern_score += 0.05\n",
    "        \n",
    "        ai_score += min(pattern_score, 0.25)\n",
    "        score_breakdown['pattern'] = min(pattern_score, 0.25)\n",
    "        \n",
    "        # === FIXED THRESHOLDS ===\n",
    "        ai_probability = min(ai_score, 1.0)\n",
    "        \n",
    "        if ai_probability >= 0.70:  # Lowered from 0.75\n",
    "            verdict = \"HIGH CONFIDENCE: Code is very likely AI-generated\"\n",
    "            risk_level = \"HIGH\"\n",
    "        elif ai_probability >= 0.55:  # Lowered from 0.60\n",
    "            verdict = \"MODERATE CONFIDENCE: Code shows significant AI patterns\"\n",
    "            risk_level = \"MEDIUM\"\n",
    "        elif ai_probability >= 0.35:  # Lowered from 0.40\n",
    "            verdict = \"LOW CONFIDENCE: Some AI-like patterns detected\"\n",
    "            risk_level = \"LOW\"\n",
    "        else:\n",
    "            verdict = \"CLEAN: Code appears human-written\"\n",
    "            risk_level = \"NONE\"\n",
    "        \n",
    "        return {\n",
    "            'ai_probability': round(ai_probability, 3),\n",
    "            'is_ai_generated': ai_probability >= 0.55,  # Lowered threshold\n",
    "            'risk_level': risk_level,\n",
    "            'verdict': verdict,\n",
    "            'score_breakdown': {\n",
    "                'perplexity_score': round(score_breakdown['perplexity'], 3),\n",
    "                'ast_score': round(score_breakdown['ast'], 3),\n",
    "                'pattern_score': round(score_breakdown['pattern'], 3)\n",
    "            },\n",
    "            'metrics': {\n",
    "                'perplexity': round(perplexity, 2),\n",
    "                'ast_features': ast_features,\n",
    "                'pattern_features': pattern_features\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae39de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plagiarism Detection Logic\n",
    "\n",
    "This section details the approach used for detecting plagiarism in code submissions:\n",
    "\n",
    "- **Common Algorithm Patterns**: The detector checks submitted code against a set of well-known algorithmic patterns (e.g., quicksort, binary search, bubble sort, merge sort, recursive Fibonacci).\n",
    "- **Hash-Based Matching**: Each pattern is pre-processed and stored as a hash for fast exact matching. If a code snippet matches a known pattern hash, it is flagged as an exact match.\n",
    "- **Similarity Analysis**: For non-exact matches, the detector uses sequence similarity to compare code structure and logic, flagging submissions with high similarity scores.\n",
    "- **Source Attribution**: When plagiarism is detected, the system reports the sources (e.g., StackOverflow, GitHub, LeetCode) where the matching pattern is commonly found.\n",
    "- **Performance**: The logic is optimized for speed using caching and early termination, making it suitable for large-scale automated code review.\n",
    "\n",
    "> This modular approach allows for easy extension with new patterns and sources, and can be integrated into backend services for real-time plagiarism detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPlagiarismDetector:\n",
    "    \"\"\"\n",
    "    Optimized plagiarism detection against common patterns\n",
    "    No training required - uses hash-based matching + similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Pre-compute hashes for faster lookup\n",
    "        self.pattern_hashes = {}\n",
    "        self.patterns_by_hash = {}\n",
    "        self._load_common_snippets()\n",
    "        \n",
    "    def _load_common_snippets(self):\n",
    "        \"\"\"Load common algorithm patterns (expandable database)\"\"\"\n",
    "        snippets = {\n",
    "            'quicksort_basic': {\n",
    "                'pattern': 'def quicksort(arr): if len(arr) <= 1: return arr',\n",
    "                'sources': ['stackoverflow', 'github']\n",
    "            },\n",
    "            'fibonacci_recursive': {\n",
    "                'pattern': 'def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2)',\n",
    "                'sources': ['common_algorithm']\n",
    "            },\n",
    "            'binary_search': {\n",
    "                'pattern': 'def binary_search(arr, target): left = 0 right = len(arr) - 1',\n",
    "                'sources': ['leetcode', 'github']\n",
    "            },\n",
    "            'bubble_sort': {\n",
    "                'pattern': 'def bubble_sort(arr): for i in range(len(arr)): for j in range(len(arr)-i-1):',\n",
    "                'sources': ['stackoverflow']\n",
    "            },\n",
    "            'merge_sort': {\n",
    "                'pattern': 'def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2',\n",
    "                'sources': ['github', 'common_algorithm']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute all hashes\n",
    "        for name, info in snippets.items():\n",
    "            pattern_hash = hashlib.md5(info['pattern'].encode()).hexdigest()\n",
    "            self.pattern_hashes[name] = pattern_hash\n",
    "            self.patterns_by_hash[pattern_hash] = {\n",
    "                'name': name,\n",
    "                'pattern': info['pattern'],\n",
    "                'sources': info['sources']\n",
    "            }\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def normalize_for_comparison(self, code):\n",
    "        \"\"\"Fast normalization for comparison\"\"\"\n",
    "        code = re.sub(r'#.*', '', code)\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', '', code)\n",
    "        code = re.sub(r'\\s+', ' ', code).strip()\n",
    "        return code\n",
    "    \n",
    "    def check_against_common_patterns(self, code):\n",
    "        \"\"\"Check against known patterns\"\"\"\n",
    "        normalized_code = self.normalize_for_comparison(code)\n",
    "        code_hash = hashlib.md5(normalized_code.encode()).hexdigest()\n",
    "        \n",
    "        # Fast exact match check\n",
    "        if code_hash in self.patterns_by_hash:\n",
    "            pattern_info = self.patterns_by_hash[code_hash]\n",
    "            return [{\n",
    "                'pattern': pattern_info['name'],\n",
    "                'similarity': 1.0,\n",
    "                'sources': pattern_info['sources'],\n",
    "                'match_type': 'exact'\n",
    "            }]\n",
    "        \n",
    "        # Similarity check\n",
    "        matches = []\n",
    "        for pattern_hash, pattern_info in self.patterns_by_hash.items():\n",
    "            len_diff = abs(len(normalized_code) - len(pattern_info['pattern']))\n",
    "            if len_diff / max(len(normalized_code), len(pattern_info['pattern'])) > 0.3:\n",
    "                continue\n",
    "            \n",
    "            similarity = difflib.SequenceMatcher(\n",
    "                None, \n",
    "                normalized_code, \n",
    "                pattern_info['pattern']\n",
    "            ).ratio()\n",
    "            \n",
    "            if similarity > 0.8:\n",
    "                matches.append({\n",
    "                    'pattern': pattern_info['name'],\n",
    "                    'similarity': similarity,\n",
    "                    'sources': pattern_info['sources'],\n",
    "                    'match_type': 'similar'\n",
    "                })\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def detect_online_plagiarism(self, code):\n",
    "        \"\"\"Detect plagiarism from known sources\"\"\"\n",
    "        matches = self.check_against_common_patterns(code)\n",
    "        \n",
    "        if matches:\n",
    "            max_similarity = max(match['similarity'] for match in matches)\n",
    "            return {\n",
    "                'is_plagiarized': max_similarity > 0.85,\n",
    "                'max_similarity': max_similarity,\n",
    "                'matches': matches,\n",
    "                'sources_found': list(set(source for match in matches for source in match['sources']))\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'is_plagiarized': False,\n",
    "            'max_similarity': 0.0,\n",
    "            'matches': [],\n",
    "            'sources_found': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4fa10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Code Analysis and Recommendation Engine\n",
    "\n",
    "This section introduces the integrated code analysis engine, which combines AI-generated code detection and plagiarism detection to provide a comprehensive assessment of code submissions:\n",
    "\n",
    "- **Parallel Processing**: The engine runs AI and plagiarism checks in parallel for faster results, making it suitable for batch analysis and large datasets.\n",
    "- **Suspiciousness Scoring**: Each code snippet is evaluated for signs of AI generation and plagiarism, with an overall suspiciousness score and detailed breakdown.\n",
    "- **Actionable Recommendations**: Based on the analysis, the engine provides clear recommendations, such as flagging high-risk submissions, suggesting manual review, or confirming clean code.\n",
    "- **Batch Support**: Multiple code samples can be analyzed simultaneously, with results aggregated for efficient review.\n",
    "- **Cache Management**: Built-in cache clearing methods help manage memory and maintain performance during repeated or large-scale analysis.\n",
    "\n",
    "> This modular recommendation engine can be integrated into automated code review systems, online judges, or educational platforms to enhance code integrity and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371bfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCodeChecker:\n",
    "    \"\"\"\n",
    "    Production-ready code checker with parallel processing\n",
    "    No training required - uses pre-trained models + rule-based analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ai_detector = OptimizedAIGeneratedCodeDetector()\n",
    "        self.plagiarism_detector = OptimizedPlagiarismDetector()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
    "    \n",
    "    def analyze_code(self, code, parallel=True):\n",
    "        \"\"\"\n",
    "        Analyze single code submission\n",
    "        \n",
    "        Args:\n",
    "            code: User-submitted code string\n",
    "            parallel: Run AI and plagiarism detection in parallel\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive analysis dict\n",
    "        \"\"\"\n",
    "        \n",
    "        if parallel:\n",
    "            # Parallel execution for speed\n",
    "            future_ai = self.executor.submit(self.ai_detector.detect_ai_generated, code)\n",
    "            future_plag = self.executor.submit(self.plagiarism_detector.detect_online_plagiarism, code)\n",
    "            \n",
    "            ai_result = future_ai.result()\n",
    "            plagiarism_result = future_plag.result()\n",
    "        else:\n",
    "            ai_result = self.ai_detector.detect_ai_generated(code)\n",
    "            plagiarism_result = self.plagiarism_detector.detect_online_plagiarism(code)\n",
    "        \n",
    "        overall_suspicious = (\n",
    "            ai_result['is_ai_generated'] or \n",
    "            plagiarism_result['is_plagiarized']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overall_suspicious': overall_suspicious,\n",
    "            'ai_detection': ai_result,\n",
    "            'plagiarism_detection': plagiarism_result,\n",
    "            'recommendation': self._get_recommendation(ai_result, plagiarism_result)\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, codes):\n",
    "        \"\"\"Batch analysis for multiple submissions\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.analyze_code, code, False) for code in codes]\n",
    "            results = [future.result() for future in futures]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_recommendation(self, ai_result, plagiarism_result):\n",
    "        \"\"\"Generate actionable recommendation\"\"\"\n",
    "        if ai_result['is_ai_generated'] and plagiarism_result['is_plagiarized']:\n",
    "            return \"ğŸš¨ CRITICAL: Code shows BOTH AI generation and plagiarism patterns\"\n",
    "        elif ai_result['is_ai_generated']:\n",
    "            prob = ai_result['ai_probability']\n",
    "            return f\"âš ï¸  AI DETECTED: Code likely AI-generated (confidence: {prob:.1%})\"\n",
    "        elif plagiarism_result['is_plagiarized']:\n",
    "            sources = ', '.join(plagiarism_result['sources_found'])\n",
    "            return f\"âš ï¸  PLAGIARISM: Matches known sources ({sources})\"\n",
    "        elif ai_result['ai_probability'] > 0.4 or plagiarism_result['max_similarity'] > 0.6:\n",
    "            return \"âš¡ MODERATE RISK: Some suspicious patterns, review recommended\"\n",
    "        else:\n",
    "            return \"âœ… CLEAN: No significant AI or plagiarism detected\"\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear LRU caches to free memory\"\"\"\n",
    "        self.ai_detector.calculate_perplexity.cache_clear()\n",
    "        self.ai_detector.extract_ast_features.cache_clear()\n",
    "        self.ai_detector.analyze_code_patterns.cache_clear()\n",
    "        self.plagiarism_detector.normalize_for_comparison.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94f412",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Cases and Performance Metrics\n",
    "\n",
    "This section provides a suite of test cases to validate the code analysis engine and benchmark its performance:\n",
    "\n",
    "- **Test Coverage**: Includes examples of suspicious AI-like code, common algorithms (potential plagiarism), and original code to demonstrate detection capabilities.\n",
    "- **Single and Batch Analysis**: Measures the time taken for individual and batch code analysis, highlighting the speedup from parallel processing.\n",
    "- **Performance Reporting**: Outputs suspiciousness, AI probability, plagiarism status, and recommendations for each test case, along with timing metrics.\n",
    "- **Cache Efficiency**: (Commented) Optionally tests cache performance for repeated analysis, showing the benefits of caching in large-scale scenarios.\n",
    "\n",
    "> Use these tests to ensure the reliability and efficiency of the detection pipeline before deploying in production or integrating with automated review systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9133888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PRODUCTION-READY AI CODE DETECTOR\n",
      "  No Training Required | Pre-trained GPT-2 + AST Analysis\n",
      "================================================================================\n",
      "\n",
      "Loading GPT-2 model on cuda...\n",
      "âœ“ GPT-2 model loaded successfully (pre-trained, no training required)!\n",
      "================================================================================\n",
      "ğŸ” COMPREHENSIVE CODE ANALYSIS TEST\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Testing single code analysis...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: ğŸ¤– Suspicious AI-like code\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ¯ VERDICT: âš¡ MODERATE RISK: Some suspicious patterns, review recommended\n",
      "   Overall Suspicious: False\n",
      "\n",
      "ğŸ¤– AI Detection:\n",
      "   Probability: 54.0%\n",
      "   Risk Level: LOW\n",
      "   Perplexity: 5.36\n",
      "   Score Breakdown:\n",
      "      - Perplexity: 0.400\n",
      "      - AST: 0.140\n",
      "      - Pattern: 0.000\n",
      "\n",
      "ğŸ“‹ Plagiarism Detection:\n",
      "   Is Plagiarized: False\n",
      "   Max Similarity: 0.0%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: ğŸ“‹ Common algorithm (plagiarism suspect)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ¯ VERDICT: âš ï¸  PLAGIARISM: Matches known sources (common_algorithm)\n",
      "   Overall Suspicious: True\n",
      "\n",
      "ğŸ¤– AI Detection:\n",
      "   Probability: 46.0%\n",
      "   Risk Level: LOW\n",
      "   Perplexity: 4.99\n",
      "   Score Breakdown:\n",
      "      - Perplexity: 0.400\n",
      "      - AST: 0.060\n",
      "      - Pattern: 0.000\n",
      "\n",
      "ğŸ“‹ Plagiarism Detection:\n",
      "   Is Plagiarized: True\n",
      "   Max Similarity: 100.0%\n",
      "   Sources: common_algorithm\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: ğŸ‘¤ Human-written code\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ¯ VERDICT: âœ… CLEAN: No significant AI or plagiarism detected\n",
      "   Overall Suspicious: False\n",
      "\n",
      "ğŸ¤– AI Detection:\n",
      "   Probability: 26.0%\n",
      "   Risk Level: NONE\n",
      "   Perplexity: 31.72\n",
      "   Score Breakdown:\n",
      "      - Perplexity: 0.200\n",
      "      - AST: 0.060\n",
      "      - Pattern: 0.000\n",
      "\n",
      "ğŸ“‹ Plagiarism Detection:\n",
      "   Is Plagiarized: False\n",
      "   Max Similarity: 0.0%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: ğŸ§ª Complex human code\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ¯ VERDICT: âš¡ MODERATE RISK: Some suspicious patterns, review recommended\n",
      "   Overall Suspicious: False\n",
      "\n",
      "ğŸ¤– AI Detection:\n",
      "   Probability: 46.0%\n",
      "   Risk Level: LOW\n",
      "   Perplexity: 7.37\n",
      "   Score Breakdown:\n",
      "      - Perplexity: 0.400\n",
      "      - AST: 0.060\n",
      "      - Pattern: 0.000\n",
      "\n",
      "ğŸ“‹ Plagiarism Detection:\n",
      "   Is Plagiarized: False\n",
      "   Max Similarity: 0.0%\n",
      "\n",
      "â±ï¸  Single analysis time: 0.28s\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ Testing batch analysis (parallel processing)...\n",
      "================================================================================\n",
      "âœ“ Processed 4 codes in 0.00s\n",
      "âœ“ Speedup: 100.89x faster\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¾ Testing cache performance (re-analyzing same code)...\n",
      "================================================================================\n",
      "âœ“ 5 cached analyses: 0.001s (0.000s per analysis)\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL TESTS COMPLETED\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TESTING & DEMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def test_optimized_checker():\n",
    "    \"\"\"Comprehensive test with performance metrics\"\"\"\n",
    "    import time\n",
    "    \n",
    "    checker = OptimizedCodeChecker()\n",
    "    \n",
    "    test_codes = [\n",
    "        {\n",
    "            'name': 'ğŸ¤– Suspicious AI-like code',\n",
    "            'code': '''\n",
    "def calculate_fibonacci_sequence(number_of_terms):\n",
    "    \"\"\"\n",
    "    Calculate fibonacci sequence up to n terms.\n",
    "    This function uses iteration to calculate fibonacci numbers efficiently.\n",
    "    \"\"\"\n",
    "    if number_of_terms <= 0:\n",
    "        return \"Please provide a positive integer\"\n",
    "    elif number_of_terms == 1:\n",
    "        return [0]\n",
    "    elif number_of_terms == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fibonacci_sequence = [0, 1]\n",
    "        for current_index in range(2, number_of_terms):\n",
    "            next_number = fibonacci_sequence[current_index - 1] + fibonacci_sequence[current_index - 2]\n",
    "            fibonacci_sequence.append(next_number)\n",
    "        return fibonacci_sequence\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ“‹ Common algorithm (plagiarism suspect)',\n",
    "            'code': '''\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ‘¤ Human-written code',\n",
    "            'code': '''\n",
    "def calc(x, y):\n",
    "    # quick calc\n",
    "    z = x + y\n",
    "    return z * 2\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ§ª Complex human code',\n",
    "            'code': '''\n",
    "def solve(arr):\n",
    "    res = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] % 2:\n",
    "            res.append(arr[i])\n",
    "    return res\n",
    "            '''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ” COMPREHENSIVE CODE ANALYSIS TEST\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Single analysis\n",
    "    print(\"\\nğŸ“Š Testing single code analysis...\\n\")\n",
    "    start = time.time()\n",
    "    \n",
    "    for test in test_codes:\n",
    "        print(f\"\\n{'â”€' * 80}\")\n",
    "        print(f\"Testing: {test['name']}\")\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "        \n",
    "        result = checker.analyze_code(test['code'])\n",
    "        \n",
    "        print(f\"\\nğŸ¯ VERDICT: {result['recommendation']}\")\n",
    "        print(f\"   Overall Suspicious: {result['overall_suspicious']}\")\n",
    "        \n",
    "        ai = result['ai_detection']\n",
    "        print(f\"\\nğŸ¤– AI Detection:\")\n",
    "        print(f\"   Probability: {ai['ai_probability']:.1%}\")\n",
    "        print(f\"   Risk Level: {ai['risk_level']}\")\n",
    "        print(f\"   Perplexity: {ai['metrics']['perplexity']:.2f}\")\n",
    "        print(f\"   Score Breakdown:\")\n",
    "        print(f\"      - Perplexity: {ai['score_breakdown']['perplexity_score']:.3f}\")\n",
    "        print(f\"      - AST: {ai['score_breakdown']['ast_score']:.3f}\")\n",
    "        print(f\"      - Pattern: {ai['score_breakdown']['pattern_score']:.3f}\")\n",
    "        \n",
    "        plag = result['plagiarism_detection']\n",
    "        print(f\"\\nğŸ“‹ Plagiarism Detection:\")\n",
    "        print(f\"   Is Plagiarized: {plag['is_plagiarized']}\")\n",
    "        print(f\"   Max Similarity: {plag['max_similarity']:.1%}\")\n",
    "        if plag['sources_found']:\n",
    "            print(f\"   Sources: {', '.join(plag['sources_found'])}\")\n",
    "    \n",
    "    single_time = time.time() - start\n",
    "    print(f\"\\nâ±ï¸  Single analysis time: {single_time:.2f}s\")\n",
    "    \n",
    "    # Batch analysis\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"ğŸš€ Testing batch analysis (parallel processing)...\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    batch_results = checker.analyze_batch([test['code'] for test in test_codes])\n",
    "    batch_time = time.time() - start\n",
    "    \n",
    "    print(f\"âœ“ Processed {len(test_codes)} codes in {batch_time:.2f}s\")\n",
    "    print(f\"âœ“ Speedup: {single_time/batch_time:.2f}x faster\")\n",
    "    \n",
    "    # Cache test\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"ğŸ’¾ Testing cache performance (re-analyzing same code)...\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(5):\n",
    "        checker.analyze_code(test_codes[0]['code'])\n",
    "    cached_time = time.time() - start\n",
    "    \n",
    "    print(f\"âœ“ 5 cached analyses: {cached_time:.3f}s ({cached_time/5:.3f}s per analysis)\")\n",
    "    #print(f\"âœ“ Cache speedup: {(single_time/len(test_codes))/(cached_time/5):.1f}x faster\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"âœ… ALL TESTS COMPLETED\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"  PRODUCTION-READY AI CODE DETECTOR\")\n",
    "    print(\"  No Training Required | Pre-trained GPT-2 + AST Analysis\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    test_optimized_checker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c3591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Saving and Deployment\n",
    "\n",
    "This section demonstrates how to save the optimized GPT-2 model and tokenizer in HuggingFace format for future use or production deployment. Saving models in this way allows for easy loading, sharing, and integration into backend services or cloud environments.\n",
    "\n",
    "- **save_dir**: The directory where the model and tokenizer will be stored.\n",
    "- **gpt2_model.save_pretrained(save_dir)**: Saves the model weights and configuration.\n",
    "- **gpt2_tokenizer.save_pretrained(save_dir)**: Saves the tokenizer files for consistent preprocessing.\n",
    "\n",
    "> After saving, you can reload the model and tokenizer using `from_pretrained(save_dir)` in any compatible environment.\n",
    "\n",
    "**Tip:** Always version your saved models and document the training or fine-tuning process for reproducibility and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in ../models\n"
     ]
    }
   ],
   "source": [
    "# Save the optimized model and tokenizer\n",
    "save_dir = \"../models\"                 # choose a folder name\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# save everything in Hugging-Face format\n",
    "gpt2_model.save_pretrained(save_dir)\n",
    "gpt2_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0850a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model on cuda...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 321\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# Simple test\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 321\u001b[0m     detector \u001b[38;5;241m=\u001b[39m \u001b[43mOptimizedAIGeneratedCodeDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     test_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124mdef calculate_fibonacci_sequence(number_of_terms):\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124m        fibonacci_sequence = [0, 1]\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124m        return fibonacci_sequence\u001b[39m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mOptimizedAIGeneratedCodeDetector.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GPT-2 model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\transformers\\modeling_utils.py:4459\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   4455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4458\u001b[0m         )\n\u001b[1;32m-> 4459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class OptimizedAIGeneratedCodeDetector:\n",
    "    \"\"\"\n",
    "    AI detection with conflict resolution between perplexity and AST\n",
    "    \"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            return\n",
    "            \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading GPT-2 model on {self.device}...\")\n",
    "        \n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
    "        self.gpt2_model.eval()\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(\"âœ“ GPT-2 model loaded successfully!\\n\")\n",
    "    \n",
    "    def calculate_perplexity(self, code):\n",
    "        \"\"\"Calculate perplexity - LOW = AI-likely\"\"\"\n",
    "        try:\n",
    "            code_lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "            code_text = ' '.join(code_lines)\n",
    "            \n",
    "            encodings = self.gpt2_tokenizer(\n",
    "                code_text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.gpt2_model(**encodings, labels=encodings['input_ids'])\n",
    "                perplexity = torch.exp(outputs.loss)\n",
    "            \n",
    "            return float(perplexity)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Perplexity calculation failed: {e}\")\n",
    "            return 150.0\n",
    "    \n",
    "    def extract_ast_features(self, code):\n",
    "        \"\"\"Extract AST features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Node analysis\n",
    "            node_types = [type(node).__name__ for node in ast.walk(tree)]\n",
    "            total_nodes = len(node_types)\n",
    "            features['total_nodes'] = total_nodes\n",
    "            features['unique_node_ratio'] = len(set(node_types)) / max(total_nodes, 1)\n",
    "            \n",
    "            # Depth analysis\n",
    "            def get_depths(node, depth=0):\n",
    "                depths = [depth]\n",
    "                for child in ast.iter_child_nodes(node):\n",
    "                    depths.extend(get_depths(child, depth + 1))\n",
    "                return depths\n",
    "            \n",
    "            depths = get_depths(tree)\n",
    "            features['max_depth'] = max(depths) if depths else 0\n",
    "            features['depth_variance'] = np.var(depths) if len(depths) > 1 else 0\n",
    "            \n",
    "            # Control flow\n",
    "            num_conditions = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.If))\n",
    "            num_loops = sum(1 for _ in ast.walk(tree) if isinstance(_, (ast.For, ast.While)))\n",
    "            num_try = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.Try))\n",
    "            \n",
    "            features['num_conditions'] = num_conditions\n",
    "            features['num_loops'] = num_loops\n",
    "            features['cyclomatic_complexity'] = 1 + num_conditions + num_loops\n",
    "            \n",
    "            # AI patterns (perfect balance, low variance)\n",
    "            features['is_perfectly_balanced'] = (\n",
    "                features['depth_variance'] < 1.0 and \n",
    "                features['cyclomatic_complexity'] < 4\n",
    "            )\n",
    "            features['has_error_handling'] = num_try > 0\n",
    "            features['parse_success'] = True\n",
    "            \n",
    "        except:\n",
    "            features['parse_success'] = False\n",
    "            features['parse_error'] = True\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def analyze_code_patterns(self, code):\n",
    "        \"\"\"Pattern analysis\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        # Comments\n",
    "        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "        docstring_count = len(re.findall(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', code))\n",
    "        features['comment_ratio'] = comment_lines / max(total_lines, 1)\n",
    "        features['has_docstrings'] = docstring_count > 0\n",
    "        \n",
    "        # Variable naming\n",
    "        try:\n",
    "            var_pattern = r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'\n",
    "            var_names = re.findall(var_pattern, code)\n",
    "            keywords = {'def', 'class', 'if', 'else', 'for', 'while', 'return', 'import'}\n",
    "            var_names = [v for v in var_names if v not in keywords]\n",
    "            \n",
    "            if var_names:\n",
    "                avg_name_length = np.mean([len(name) for name in var_names])\n",
    "                long_names = sum(1 for name in var_names if len(name) >= 10)\n",
    "                \n",
    "                features['avg_var_name_length'] = avg_name_length\n",
    "                features['long_name_ratio'] = long_names / len(var_names)\n",
    "            else:\n",
    "                features['avg_var_name_length'] = 0\n",
    "                features['long_name_ratio'] = 0\n",
    "        except:\n",
    "            features['avg_var_name_length'] = 0\n",
    "            features['long_name_ratio'] = 0\n",
    "        \n",
    "        # Indentation\n",
    "        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]\n",
    "        if indents:\n",
    "            features['indent_consistency'] = 1 - (np.std(indents) / (np.mean(indents) + 1))\n",
    "        else:\n",
    "            features['indent_consistency'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def detect_ai_generated(self, code):\n",
    "        \"\"\"\n",
    "        FIXED: Perplexity-first approach with AST as secondary\n",
    "        \n",
    "        Key insight: Perplexity is the STRONGEST indicator for AI detection\n",
    "        AST features can have false negatives, so we weight perplexity higher\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate features\n",
    "        perplexity = self.calculate_perplexity(code)\n",
    "        ast_features = self.extract_ast_features(code)\n",
    "        pattern_features = self.analyze_code_patterns(code)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"DETAILED ANALYSIS:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # PERPLEXITY ANALYSIS (PRIMARY INDICATOR - 60% weight)\n",
    "        # Research: Most reliable single metric for AI detection\n",
    "        # ================================================================\n",
    "        \n",
    "        print(f\"\\n1ï¸âƒ£  PERPLEXITY ANALYSIS\")\n",
    "        print(f\"    Value: {perplexity:.2f}\")\n",
    "        \n",
    "        if perplexity < 10:\n",
    "            perplexity_score = 0.60\n",
    "            perplexity_level = \"CRITICAL\"\n",
    "            interpretation = \"EXTREMELY LOW - Very strong AI signature\"\n",
    "        elif perplexity < 20:\n",
    "            perplexity_score = 0.50\n",
    "            perplexity_level = \"HIGH\"\n",
    "            interpretation = \"Very low - Strong AI indicator\"\n",
    "        elif perplexity < 35:\n",
    "            perplexity_score = 0.35\n",
    "            perplexity_level = \"MODERATE-HIGH\"\n",
    "            interpretation = \"Low - Significant AI suspicion\"\n",
    "        elif perplexity < 60:\n",
    "            perplexity_score = 0.20\n",
    "            perplexity_level = \"MODERATE\"\n",
    "            interpretation = \"Moderate - Some AI patterns\"\n",
    "        elif perplexity < 100:\n",
    "            perplexity_score = 0.10\n",
    "            perplexity_level = \"LOW\"\n",
    "            interpretation = \"Higher - Leaning human\"\n",
    "        else:\n",
    "            perplexity_score = 0.0\n",
    "            perplexity_level = \"NONE\"\n",
    "            interpretation = \"High - Human-like variability\"\n",
    "        \n",
    "        print(f\"    Level: {perplexity_level}\")\n",
    "        print(f\"    {interpretation}\")\n",
    "        print(f\"    Score Contribution: {perplexity_score:.3f} / 0.600\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # AST STRUCTURE ANALYSIS (SECONDARY - 25% weight)\n",
    "        # Can have false negatives, so lower weight\n",
    "        # ================================================================\n",
    "        \n",
    "        print(f\"\\n2ï¸âƒ£  AST STRUCTURE ANALYSIS\")\n",
    "        print(f\"    Balanced: {ast_features.get('is_perfectly_balanced', False)}\")\n",
    "        print(f\"    Depth Variance: {ast_features.get('depth_variance', 0):.2f}\")\n",
    "        print(f\"    Complexity: {ast_features.get('cyclomatic_complexity', 0)}\")\n",
    "        \n",
    "        ast_score = 0.0\n",
    "        \n",
    "        if not ast_features.get('parse_error', False):\n",
    "            # Perfect balance (strong AI indicator)\n",
    "            if ast_features.get('is_perfectly_balanced', False):\n",
    "                ast_score += 0.15\n",
    "                print(f\"    âœ“ Perfect balance detected (+0.15)\")\n",
    "            \n",
    "            # Low depth variance (AI pattern)\n",
    "            if ast_features.get('depth_variance', 10) < 1.5:\n",
    "                ast_score += 0.10\n",
    "                print(f\"    âœ“ Low depth variance (+0.10)\")\n",
    "            \n",
    "            # Very low node diversity\n",
    "            if ast_features.get('unique_node_ratio', 1.0) < 0.35:\n",
    "                ast_score += 0.05\n",
    "        \n",
    "        ast_score = min(ast_score, 0.25)\n",
    "        print(f\"    Score Contribution: {ast_score:.3f} / 0.250\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # PATTERN ANALYSIS (TERTIARY - 15% weight)\n",
    "        # ================================================================\n",
    "        \n",
    "        print(f\"\\n3ï¸âƒ£  PATTERN ANALYSIS\")\n",
    "        print(f\"    Avg Variable Length: {pattern_features.get('avg_var_name_length', 0):.1f}\")\n",
    "        print(f\"    Comment Ratio: {pattern_features.get('comment_ratio', 0):.2f}\")\n",
    "        print(f\"    Indent Consistency: {pattern_features.get('indent_consistency', 0):.2f}\")\n",
    "        \n",
    "        pattern_score = 0.0\n",
    "        \n",
    "        # Very long descriptive names (AI loves these)\n",
    "        if pattern_features.get('avg_var_name_length', 0) > 15:\n",
    "            pattern_score += 0.08\n",
    "            print(f\"    âœ“ Very long variable names (+0.08)\")\n",
    "        elif pattern_features.get('avg_var_name_length', 0) > 10:\n",
    "            pattern_score += 0.04\n",
    "        \n",
    "        # Excessive documentation\n",
    "        if pattern_features.get('has_docstrings', False) and pattern_features.get('comment_ratio', 0) > 0.3:\n",
    "            pattern_score += 0.07\n",
    "            print(f\"    âœ“ Excessive documentation (+0.07)\")\n",
    "        \n",
    "        # Perfect indentation\n",
    "        if pattern_features.get('indent_consistency', 0) > 0.98:\n",
    "            pattern_score += 0.05\n",
    "        \n",
    "        pattern_score = min(pattern_score, 0.15)\n",
    "        print(f\"    Score Contribution: {pattern_score:.3f} / 0.150\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # FINAL SCORING\n",
    "        # ================================================================\n",
    "        \n",
    "        total_score = perplexity_score + ast_score + pattern_score\n",
    "        ai_probability = min(total_score, 1.0)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TOTAL AI PROBABILITY: {ai_probability:.3f} ({ai_probability*100:.1f}%)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Determine verdict\n",
    "        if ai_probability >= 0.65:\n",
    "            verdict = \"HIGH CONFIDENCE: Code is very likely AI-generated\"\n",
    "            risk_level = \"HIGH\"\n",
    "        elif ai_probability >= 0.45:\n",
    "            verdict = \"MODERATE CONFIDENCE: Code shows significant AI patterns\"\n",
    "            risk_level = \"MEDIUM\"\n",
    "        elif ai_probability >= 0.30:\n",
    "            verdict = \"LOW CONFIDENCE: Some AI-like patterns detected\"\n",
    "            risk_level = \"LOW\"\n",
    "        else:\n",
    "            verdict = \"CLEAN: Code appears human-written\"\n",
    "            risk_level = \"NONE\"\n",
    "        \n",
    "        # Conflict detection\n",
    "        conflict_detected = False\n",
    "        if perplexity < 15 and ast_score < 0.10:\n",
    "            conflict_detected = True\n",
    "            verdict += \" [âš ï¸  CONFLICT: Low perplexity but human-like structure]\"\n",
    "            print(f\"\\nâš ï¸  CONFLICT DETECTED:\")\n",
    "            print(f\"    Perplexity signals strong AI ({perplexity:.2f})\")\n",
    "            print(f\"    But AST structure appears human-like\")\n",
    "            print(f\"    This may indicate: Common code patterns, or AI with unusual structure\")\n",
    "        \n",
    "        return {\n",
    "            'ai_probability': round(ai_probability, 3),\n",
    "            'is_ai_generated': ai_probability >= 0.45,\n",
    "            'risk_level': risk_level,\n",
    "            'verdict': verdict,\n",
    "            'conflict_detected': conflict_detected,\n",
    "            'score_breakdown': {\n",
    "                'perplexity_score': round(perplexity_score, 3),\n",
    "                'ast_score': round(ast_score, 3),\n",
    "                'pattern_score': round(pattern_score, 3)\n",
    "            },\n",
    "            'metrics': {\n",
    "                'perplexity': round(perplexity, 2),\n",
    "                'perplexity_level': perplexity_level,\n",
    "                'ast_features': ast_features,\n",
    "                'pattern_features': pattern_features\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Simple test\n",
    "if __name__ == \"__main__\":\n",
    "    detector = OptimizedAIGeneratedCodeDetector()\n",
    "    \n",
    "    test_code = '''\n",
    "def calculate_fibonacci_sequence(number_of_terms):\n",
    "        fibonacci_sequence = [0, 1]\n",
    "        for current_index in range(2, number_of_terms):\n",
    "            next_number = fibonacci_sequence[current_index - 1] + fibonacci_sequence[current_index - 2]\n",
    "            fibonacci_sequence.append(next_number)\n",
    "        return fibonacci_sequence\n",
    "    '''\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TESTING: AI-like Fibonacci Code\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = detector.detect_ai_generated(test_code)\n",
    "    \n",
    "    print(f\"\\n\\n{'ğŸ¯ FINAL VERDICT':^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Verdict: {result['verdict']}\")\n",
    "    print(f\"AI Probability: {result['ai_probability']:.1%}\")\n",
    "    print(f\"Risk Level: {result['risk_level']}\")\n",
    "    print(f\"Is AI Generated: {result['is_ai_generated']}\")\n",
    "    print(f\"Conflict Detected: {result['conflict_detected']}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your test file\n",
    "\n",
    "def comprehensive_test_suite():\n",
    "    \"\"\"Test suite with diverse code examples\"\"\"\n",
    "    \n",
    "    detector = OptimizedAIGeneratedCodeDetector()\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            'name': 'ğŸ¤– CASE 1: Classic AI-Generated (ChatGPT style)',\n",
    "            'expected': 'HIGH AI (80-95%)',\n",
    "            'code': '''\n",
    "def calculate_prime_numbers(upper_limit):\n",
    "    \"\"\"\n",
    "    Generate all prime numbers up to the specified upper limit.\n",
    "    \n",
    "    This function implements the Sieve of Eratosthenes algorithm\n",
    "    to efficiently find all prime numbers up to upper_limit.\n",
    "    \n",
    "    Args:\n",
    "        upper_limit (int): The upper boundary for prime number generation\n",
    "        \n",
    "    Returns:\n",
    "        list: A list containing all prime numbers up to upper_limit\n",
    "    \"\"\"\n",
    "    if upper_limit < 2:\n",
    "        return []\n",
    "    \n",
    "    is_prime = [True] * (upper_limit + 1)\n",
    "    is_prime[0] = is_prime[1] = False\n",
    "    \n",
    "    for current_number in range(2, int(upper_limit ** 0.5) + 1):\n",
    "        if is_prime[current_number]:\n",
    "            for multiple in range(current_number * current_number, upper_limit + 1, current_number):\n",
    "                is_prime[multiple] = False\n",
    "    \n",
    "    prime_numbers = [number for number, is_prime_flag in enumerate(is_prime) if is_prime_flag]\n",
    "    return prime_numbers\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'ğŸ‘¤ CASE 2: Human-Written (Messy, Quick Code)',\n",
    "            'expected': 'LOW/CLEAN (0-30%)',\n",
    "            'code': '''\n",
    "def primes(n):\n",
    "    # quick prime check\n",
    "    res=[]\n",
    "    for i in range(2,n):\n",
    "        ok=True\n",
    "        for j in range(2,i):\n",
    "            if i%j==0:\n",
    "                ok=False\n",
    "                break\n",
    "        if ok:res.append(i)\n",
    "    return res\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'ğŸ”„ CASE 3: Human Code with AI Refactoring',\n",
    "            'expected': 'MEDIUM (45-65%)',\n",
    "            'code': '''\n",
    "def find_duplicates(array):\n",
    "    \"\"\"Find duplicate elements in array\"\"\"\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    \n",
    "    for element in array:\n",
    "        if element in seen:\n",
    "            if element not in duplicates:\n",
    "                duplicates.append(element)\n",
    "        else:\n",
    "            seen.add(element)\n",
    "    \n",
    "    return duplicates\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'ğŸ§ª CASE 4: Competitive Programming Style (Human)',\n",
    "            'expected': 'LOW/MEDIUM (20-40%)',\n",
    "            'code': '''\n",
    "def solve():\n",
    "    n = int(input())\n",
    "    a = list(map(int, input().split()))\n",
    "    \n",
    "    # dp[i] = max sum ending at i\n",
    "    dp = [0]*n\n",
    "    dp[0] = max(0, a[0])\n",
    "    \n",
    "    for i in range(1,n):\n",
    "        dp[i] = max(dp[i-1]+a[i], a[i], 0)\n",
    "    \n",
    "    print(max(dp))\n",
    "\n",
    "solve()\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': ' CASE 5: AI with Deliberate \"Humanization\"',\n",
    "            'expected': 'MEDIUM-HIGH (55-75%)',\n",
    "            'code': '''\n",
    "def binary_search(arr, target):\n",
    "    \"\"\"Binary search implementation\"\"\"\n",
    "    left, right = 0, len(arr) - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1  # not found\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'ğŸ’€ CASE 6: Copy-Pasted from StackOverflow',\n",
    "            'expected': 'MEDIUM (40-60%)',\n",
    "            'code': '''\n",
    "def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "            '''\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'ğŸ”¥ CASE 7: Expert Human (Clean but Natural)',\n",
    "            'expected': 'LOW (10-35%)',\n",
    "            'code': '''\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    \n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "    \n",
    "    for current in intervals[1:]:\n",
    "        last = merged[-1]\n",
    "        \n",
    "        if current[0] <= last[1]:\n",
    "            merged[-1] = [last[0], max(last[1], current[1])]\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    \n",
    "    return merged\n",
    "            '''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ§ª COMPREHENSIVE AI DETECTION TEST SUITE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for idx, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\n\\n{'='*80}\")\n",
    "        print(f\"TEST {idx}/7: {test['name']}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = detector.detect_ai_generated(test['code'])\n",
    "        \n",
    "        print(f\"\\n\\n{' RESULTS':^80}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"AI Probability: {result['ai_probability']:.1%}\")\n",
    "        print(f\"Risk Level: {result['risk_level']}\")\n",
    "        print(f\"Verdict: {result['verdict']}\")\n",
    "        print(f\"Is AI Generated: {result['is_ai_generated']}\")\n",
    "        print(f\"Conflict Detected: {result.get('conflict_detected', False)}\")\n",
    "        \n",
    "        print(f\"\\ Score Breakdown:\")\n",
    "        print(f\"   Perplexity: {result['score_breakdown']['perplexity_score']:.3f}\")\n",
    "        print(f\"   AST: {result['score_breakdown']['ast_score']:.3f}\")\n",
    "        print(f\"   Pattern: {result['score_breakdown']['pattern_score']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n Detailed Metrics:\")\n",
    "        print(f\"   Perplexity Value: {result['metrics']['perplexity']:.2f}\")\n",
    "        print(f\"   Perplexity Level: {result['metrics']['perplexity_level']}\")\n",
    "        \n",
    "        # Store for summary\n",
    "        results_summary.append({\n",
    "            'name': test['name'],\n",
    "            'expected': test['expected'],\n",
    "            'probability': result['ai_probability'],\n",
    "            'verdict': result['risk_level'],\n",
    "            'perplexity': result['metrics']['perplexity']\n",
    "        })\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Pause between tests for readability\n",
    "        import time\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"ğŸ“‹ TEST RESULTS SUMMARY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"{'Test':<45} {'Expected':<20} {'Actual':<15} {'Perplexity':<12}\")\n",
    "    print(f\"{'-'*45} {'-'*20} {'-'*15} {'-'*12}\")\n",
    "    \n",
    "    for r in results_summary:\n",
    "        name_short = r['name'][:43]\n",
    "        print(f\"{name_short:<45} {r['expected']:<20} {r['probability']*100:>6.1f}% ({r['verdict']:<4}) {r['perplexity']:>8.2f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" All tests completed!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_test_suite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11450d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Code Analysis System...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OptimizedAIGeneratedCodeDetector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 531\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 531\u001b[0m     checker \u001b[38;5;241m=\u001b[39m \u001b[43mOptimizedCodeChecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m     test_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;124mdef bubble_sort(arr):\u001b[39m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124m    for i in range(len(arr)):\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124m    return arr\u001b[39m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 418\u001b[0m, in \u001b[0;36mOptimizedCodeChecker.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Code Analysis System...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_detector \u001b[38;5;241m=\u001b[39m \u001b[43mOptimizedAIGeneratedCodeDetector\u001b[49m()\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplagiarism_detector \u001b[38;5;241m=\u001b[39m EnhancedPlagiarismDetector()\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OptimizedAIGeneratedCodeDetector' is not defined"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import difflib\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "class OptimizedAIGeneratedCodeDetector:\n",
    "    \"\"\"\n",
    "    Advanced normalization for plagiarism detection\n",
    "    Removes superficial differences while preserving semantic structure\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_code(code: str, level: str = 'medium') -> str:\n",
    "        \"\"\"\n",
    "        Multi-level normalization\n",
    "        \n",
    "        Args:\n",
    "            code: Source code string\n",
    "            level: 'light', 'medium', 'aggressive'\n",
    "        \"\"\"\n",
    "        if level == 'light':\n",
    "            return OptimizedAIGeneratedCodeDetector._light_normalize(code)\n",
    "        elif level == 'medium':\n",
    "            return OptimizedAIGeneratedCodeDetector._medium_normalize(code)\n",
    "        else:\n",
    "            return OptimizedAIGeneratedCodeDetector._aggressive_normalize(code)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _light_normalize(code: str) -> str:\n",
    "        \"\"\"Remove comments and extra whitespace only\"\"\"\n",
    "        # Remove single-line comments\n",
    "        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)\n",
    "        # Remove docstrings\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code)\n",
    "        code = re.sub(r\"'''[\\s\\S]*?'''\", '', code)\n",
    "        # Normalize whitespace\n",
    "        code = re.sub(r'\\s+', ' ', code).strip()\n",
    "        return code\n",
    "    \n",
    "    @staticmethod\n",
    "    def _medium_normalize(code: str) -> str:\n",
    "        \"\"\"Remove comments, normalize whitespace and variable names\"\"\"\n",
    "        code = OptimizedAIGeneratedCodeDetector._light_normalize(code)\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            class VariableRenamer(ast.NodeTransformer):\n",
    "                def __init__(self):\n",
    "                    self.var_map = {}\n",
    "                    self.counter = 0\n",
    "                    self.builtins = {'print', 'len', 'range', 'str', 'int', 'float', \n",
    "                                   'list', 'dict', 'set', 'sum', 'max', 'min', 'input'}\n",
    "                \n",
    "                def visit_Name(self, node):\n",
    "                    if node.id not in self.builtins:\n",
    "                        if node.id not in self.var_map:\n",
    "                            self.var_map[node.id] = f'v{self.counter}'\n",
    "                            self.counter += 1\n",
    "                        node.id = self.var_map[node.id]\n",
    "                    return node\n",
    "                \n",
    "                def visit_FunctionDef(self, node):\n",
    "                    if node.name not in self.var_map:\n",
    "                        self.var_map[node.name] = f'f{self.counter}'\n",
    "                        self.counter += 1\n",
    "                    node.name = self.var_map[node.name]\n",
    "                    self.generic_visit(node)\n",
    "                    return node\n",
    "            \n",
    "            renamer = VariableRenamer()\n",
    "            tree = renamer.visit(tree)\n",
    "            code = ast.unparse(tree)\n",
    "        except:\n",
    "            pass  # Return light normalized if AST fails\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    @staticmethod\n",
    "    def _aggressive_normalize(code: str) -> str:\n",
    "        \"\"\"Full normalization including structure simplification\"\"\"\n",
    "        code = OptimizedAIGeneratedCodeDetector._medium_normalize(code)\n",
    "        \n",
    "        # Additional aggressive normalizations\n",
    "        code = re.sub(r'\\s+', '', code)  # Remove ALL whitespace\n",
    "        code = code.lower()  # Lowercase everything\n",
    "        \n",
    "        return code\n",
    "\n",
    "\n",
    "class EnhancedPlagiarismDetector:\n",
    "    \"\"\"\n",
    "    Production-grade plagiarism detection with multiple strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pattern_db_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize plagiarism detector\n",
    "        \n",
    "        Args:\n",
    "            pattern_db_path: Path to custom pattern database (optional)\n",
    "        \"\"\"\n",
    "        self.normalizer = OptimizedAIGeneratedCodeDetector()\n",
    "        self.pattern_hashes = {}\n",
    "        self.patterns_by_hash = {}\n",
    "        self.pattern_database = {}\n",
    "        \n",
    "        # Load common patterns\n",
    "        self._load_common_patterns()\n",
    "        \n",
    "        # Load custom patterns if provided\n",
    "        if pattern_db_path:\n",
    "            self._load_custom_patterns(pattern_db_path)\n",
    "    \n",
    "    def _load_common_patterns(self):\n",
    "        \"\"\"Load expanded set of common algorithm patterns\"\"\"\n",
    "        \n",
    "        patterns = {\n",
    "            # Sorting algorithms\n",
    "            'quicksort_basic': {\n",
    "                'code': 'def quicksort(arr): if len(arr) <= 1: return arr pivot = arr[len(arr) // 2]',\n",
    "                'category': 'sorting',\n",
    "                'sources': ['stackoverflow', 'github', 'geeksforgeeks']\n",
    "            },\n",
    "            'bubble_sort': {\n",
    "                'code': 'def bubble_sort(arr): for i in range(len(arr)): for j in range(len(arr)-i-1): if arr[j] > arr[j+1]:',\n",
    "                'category': 'sorting',\n",
    "                'sources': ['stackoverflow', 'w3schools']\n",
    "            },\n",
    "            'merge_sort': {\n",
    "                'code': 'def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2 left = arr[:mid]',\n",
    "                'category': 'sorting',\n",
    "                'sources': ['github', 'programiz']\n",
    "            },\n",
    "            'insertion_sort': {\n",
    "                'code': 'def insertion_sort(arr): for i in range(1, len(arr)): key = arr[i] j = i-1',\n",
    "                'category': 'sorting',\n",
    "                'sources': ['stackoverflow']\n",
    "            },\n",
    "            \n",
    "            # Searching algorithms\n",
    "            'binary_search': {\n",
    "                'code': 'def binary_search(arr, target): left = 0 right = len(arr) - 1 while left <= right:',\n",
    "                'category': 'searching',\n",
    "                'sources': ['leetcode', 'github']\n",
    "            },\n",
    "            'linear_search': {\n",
    "                'code': 'def linear_search(arr, target): for i in range(len(arr)): if arr[i] == target: return i',\n",
    "                'category': 'searching',\n",
    "                'sources': ['stackoverflow']\n",
    "            },\n",
    "            \n",
    "            # Dynamic programming\n",
    "            'fibonacci_recursive': {\n",
    "                'code': 'def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2)',\n",
    "                'category': 'dynamic_programming',\n",
    "                'sources': ['common_algorithm', 'geeksforgeeks']\n",
    "            },\n",
    "            'fibonacci_dp': {\n",
    "                'code': 'def fibonacci(n): dp = [0, 1] for i in range(2, n+1): dp.append(dp[i-1] + dp[i-2])',\n",
    "                'category': 'dynamic_programming',\n",
    "                'sources': ['leetcode']\n",
    "            },\n",
    "            'knapsack': {\n",
    "                'code': 'def knapsack(weights, values, capacity): dp = [[0 for _ in range(capacity+1)] for _ in range(len(weights)+1)]',\n",
    "                'category': 'dynamic_programming',\n",
    "                'sources': ['geeksforgeeks', 'github']\n",
    "            },\n",
    "            \n",
    "            # Data structures\n",
    "            'linked_list_node': {\n",
    "                'code': 'class Node: def __init__(self, data): self.data = data self.next = None',\n",
    "                'category': 'data_structures',\n",
    "                'sources': ['stackoverflow', 'github']\n",
    "            },\n",
    "            'binary_tree_node': {\n",
    "                'code': 'class TreeNode: def __init__(self, val): self.val = val self.left = None self.right = None',\n",
    "                'category': 'data_structures',\n",
    "                'sources': ['leetcode', 'github']\n",
    "            },\n",
    "            \n",
    "            # Graph algorithms\n",
    "            'dfs_recursive': {\n",
    "                'code': 'def dfs(graph, node, visited): if node not in visited: visited.add(node) for neighbor in graph[node]:',\n",
    "                'category': 'graph',\n",
    "                'sources': ['geeksforgeeks']\n",
    "            },\n",
    "            'bfs': {\n",
    "                'code': 'def bfs(graph, start): visited = set() queue = [start] while queue: node = queue.pop(0)',\n",
    "                'category': 'graph',\n",
    "                'sources': ['stackoverflow', 'github']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute hashes for all patterns\n",
    "        for name, info in patterns.items():\n",
    "            normalized = self.normalizer.normalize_code(info['code'], level='medium')\n",
    "            pattern_hash = hashlib.md5(normalized.encode()).hexdigest()\n",
    "            \n",
    "            self.pattern_hashes[name] = pattern_hash\n",
    "            self.patterns_by_hash[pattern_hash] = {\n",
    "                'name': name,\n",
    "                'code': info['code'],\n",
    "                'normalized': normalized,\n",
    "                'category': info['category'],\n",
    "                'sources': info['sources']\n",
    "            }\n",
    "            self.pattern_database[name] = info\n",
    "    \n",
    "    def _load_custom_patterns(self, db_path: str):\n",
    "        \"\"\"Load custom patterns from file/database\"\"\"\n",
    "        # Placeholder for custom pattern loading\n",
    "        # In production, this would load from a database or file\n",
    "        pass\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def _cached_normalize(self, code: str, level: str) -> str:\n",
    "        \"\"\"Cached normalization for performance\"\"\"\n",
    "        return self.normalizer.normalize_code(code, level)\n",
    "    \n",
    "    def check_pattern_database(self, code: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Check code against known pattern database\n",
    "        \n",
    "        Returns:\n",
    "            Dict with matches and similarity scores\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        # Try multiple normalization levels\n",
    "        for level in ['medium', 'aggressive']:\n",
    "            normalized_code = self._cached_normalize(code, level)\n",
    "            code_hash = hashlib.md5(normalized_code.encode()).hexdigest()\n",
    "            \n",
    "            # Exact hash match\n",
    "            if code_hash in self.patterns_by_hash:\n",
    "                pattern_info = self.patterns_by_hash[code_hash]\n",
    "                matches.append({\n",
    "                    'pattern_name': pattern_info['name'],\n",
    "                    'category': pattern_info['category'],\n",
    "                    'similarity': 1.0,\n",
    "                    'sources': pattern_info['sources'],\n",
    "                    'match_type': 'exact',\n",
    "                    'normalization_level': level\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy matching for similar patterns\n",
    "            for pattern_hash, pattern_info in self.patterns_by_hash.items():\n",
    "                pattern_normalized = pattern_info['normalized']\n",
    "                \n",
    "                # Quick length check to skip obviously different codes\n",
    "                len_ratio = len(normalized_code) / max(len(pattern_normalized), 1)\n",
    "                if len_ratio < 0.5 or len_ratio > 2.0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = difflib.SequenceMatcher(\n",
    "                    None, \n",
    "                    normalized_code, \n",
    "                    pattern_normalized\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > 0.75:  # Threshold for similarity\n",
    "                    matches.append({\n",
    "                        'pattern_name': pattern_info['name'],\n",
    "                        'category': pattern_info['category'],\n",
    "                        'similarity': similarity,\n",
    "                        'sources': pattern_info['sources'],\n",
    "                        'match_type': 'similar',\n",
    "                        'normalization_level': level\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicate matches\n",
    "        seen = set()\n",
    "        unique_matches = []\n",
    "        for match in matches:\n",
    "            key = (match['pattern_name'], match['match_type'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_matches.append(match)\n",
    "        \n",
    "        return unique_matches\n",
    "    \n",
    "    def compare_submissions(self, code1: str, code2: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare two code submissions for similarity\n",
    "        \n",
    "        Args:\n",
    "            code1: First code submission\n",
    "            code2: Second code submission\n",
    "            \n",
    "        Returns:\n",
    "            Dict with similarity analysis\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Compare at multiple normalization levels\n",
    "        for level in ['light', 'medium', 'aggressive']:\n",
    "            norm1 = self._cached_normalize(code1, level)\n",
    "            norm2 = self._cached_normalize(code2, level)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()\n",
    "            \n",
    "            results[f'{level}_similarity'] = similarity\n",
    "        \n",
    "        # Structural similarity using AST\n",
    "        try:\n",
    "            tree1 = ast.parse(code1)\n",
    "            tree2 = ast.parse(code2)\n",
    "            \n",
    "            # Compare AST structures\n",
    "            dump1 = ast.dump(tree1)\n",
    "            dump2 = ast.dump(tree2)\n",
    "            \n",
    "            structural_similarity = difflib.SequenceMatcher(None, dump1, dump2).ratio()\n",
    "            results['structural_similarity'] = structural_similarity\n",
    "        except:\n",
    "            results['structural_similarity'] = 0.0\n",
    "        \n",
    "        # Overall assessment\n",
    "        results['max_similarity'] = max(\n",
    "            results.get('light_similarity', 0),\n",
    "            results.get('medium_similarity', 0),\n",
    "            results.get('aggressive_similarity', 0)\n",
    "        )\n",
    "        \n",
    "        results['is_similar'] = results['max_similarity'] > 0.85\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def detect_plagiarism(self, code: str, submission_database: Optional[List[Dict]] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive plagiarism detection\n",
    "        \n",
    "        Args:\n",
    "            code: Code to analyze\n",
    "            submission_database: Optional list of previous submissions to compare against\n",
    "                               Each dict should have {'code': str, 'user_id': str, 'timestamp': str}\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive plagiarism analysis\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'pattern_matches': [],\n",
    "            'submission_matches': [],\n",
    "            'sources_found': [],\n",
    "            'risk_level': 'NONE'\n",
    "        }\n",
    "        \n",
    "        # Check against common patterns\n",
    "        pattern_matches = self.check_pattern_database(code)\n",
    "        results['pattern_matches'] = pattern_matches\n",
    "        \n",
    "        if pattern_matches:\n",
    "            max_pattern_similarity = max(m['similarity'] for m in pattern_matches)\n",
    "            results['sources_found'] = list(set(\n",
    "                source for match in pattern_matches for source in match['sources']\n",
    "            ))\n",
    "            \n",
    "            # Determine if pattern match is problematic\n",
    "            if max_pattern_similarity >= 0.95:\n",
    "                results['is_plagiarized'] = True\n",
    "                results['confidence'] = max_pattern_similarity\n",
    "                results['risk_level'] = 'HIGH'\n",
    "                results['reason'] = f\"Exact copy of common pattern\"\n",
    "            elif max_pattern_similarity >= 0.85:\n",
    "                results['is_plagiarized'] = True\n",
    "                results['confidence'] = max_pattern_similarity\n",
    "                results['risk_level'] = 'MEDIUM'\n",
    "                results['reason'] = f\"Very similar to common pattern\"\n",
    "        \n",
    "        # Check against submission database if provided\n",
    "        if submission_database:\n",
    "            for submission in submission_database:\n",
    "                comparison = self.compare_submissions(code, submission['code'])\n",
    "                \n",
    "                if comparison['max_similarity'] > 0.85:\n",
    "                    results['submission_matches'].append({\n",
    "                        'user_id': submission.get('user_id', 'unknown'),\n",
    "                        'timestamp': submission.get('timestamp', 'unknown'),\n",
    "                        'similarity': comparison['max_similarity'],\n",
    "                        'details': comparison\n",
    "                    })\n",
    "                    \n",
    "                    if comparison['max_similarity'] > results['confidence']:\n",
    "                        results['is_plagiarized'] = True\n",
    "                        results['confidence'] = comparison['max_similarity']\n",
    "                        results['risk_level'] = 'HIGH' if comparison['max_similarity'] > 0.95 else 'MEDIUM'\n",
    "                        results['reason'] = f\"Matches another submission\"\n",
    "        \n",
    "        # Final assessment\n",
    "        if not results['is_plagiarized'] and results['pattern_matches']:\n",
    "            # Some similarity but not plagiarism\n",
    "            avg_similarity = np.mean([m['similarity'] for m in pattern_matches])\n",
    "            if avg_similarity > 0.6:\n",
    "                results['risk_level'] = 'LOW'\n",
    "                results['reason'] = \"Uses common algorithm patterns (acceptable)\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class OptimizedCodeChecker:\n",
    "    \"\"\"\n",
    "    Unified code analysis system with AI detection and plagiarism checking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing Code Analysis System...\")\n",
    "        self.ai_detector = OptimizedAIGeneratedCodeDetector()\n",
    "        self.plagiarism_detector = EnhancedPlagiarismDetector()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
    "        print(\"Code Analysis System Ready\\n\")\n",
    "    \n",
    "    def analyze_code(self, \n",
    "                    code: str, \n",
    "                    submission_database: Optional[List[Dict]] = None,\n",
    "                    parallel: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive code analysis\n",
    "        \n",
    "        Args:\n",
    "            code: Code to analyze\n",
    "            submission_database: Optional previous submissions for plagiarism comparison\n",
    "            parallel: Run detections in parallel\n",
    "        \n",
    "        Returns:\n",
    "            Complete analysis results\n",
    "        \"\"\"\n",
    "        \n",
    "        if parallel:\n",
    "            # Run both detections in parallel\n",
    "            future_ai = self.executor.submit(self.ai_detector.detect_ai_generated, code)\n",
    "            future_plag = self.executor.submit(\n",
    "                self.plagiarism_detector.detect_plagiarism, \n",
    "                code, \n",
    "                submission_database\n",
    "            )\n",
    "            \n",
    "            ai_result = future_ai.result()\n",
    "            plag_result = future_plag.result()\n",
    "        else:\n",
    "            ai_result = self.ai_detector.detect_ai_generated(code)\n",
    "            plag_result = self.plagiarism_detector.detect_plagiarism(code, submission_database)\n",
    "        \n",
    "        # Combine results\n",
    "        overall_suspicious = (\n",
    "            ai_result['is_ai_generated'] or \n",
    "            plag_result['is_plagiarized']\n",
    "        )\n",
    "        \n",
    "        # Determine overall risk level\n",
    "        risk_levels = {'NONE': 0, 'LOW': 1, 'MEDIUM': 2, 'HIGH': 3, 'CRITICAL': 4}\n",
    "        ai_risk = risk_levels.get(ai_result['risk_level'], 0)\n",
    "        plag_risk = risk_levels.get(plag_result['risk_level'], 0)\n",
    "        \n",
    "        overall_risk_value = max(ai_risk, plag_risk)\n",
    "        overall_risk = [k for k, v in risk_levels.items() if v == overall_risk_value][0]\n",
    "        \n",
    "        return {\n",
    "            'overall_suspicious': overall_suspicious,\n",
    "            'overall_risk_level': overall_risk,\n",
    "            'ai_detection': ai_result,\n",
    "            'plagiarism_detection': plag_result,\n",
    "            'recommendation': self._generate_recommendation(ai_result, plag_result),\n",
    "            'action': self._determine_action(ai_result, plag_result)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendation(self, ai_result: Dict, plag_result: Dict) -> str:\n",
    "        \"\"\"Generate human-readable recommendation\"\"\"\n",
    "        \n",
    "        if ai_result['is_ai_generated'] and plag_result['is_plagiarized']:\n",
    "            return (f\"CRITICAL: Code shows both AI generation \"\n",
    "                   f\"(confidence: {ai_result['ai_probability']:.1%}) \"\n",
    "                   f\"and plagiarism (similarity: {plag_result['confidence']:.1%})\")\n",
    "        \n",
    "        elif ai_result['is_ai_generated']:\n",
    "            return (f\"AI DETECTED: Code likely AI-generated \"\n",
    "                   f\"(confidence: {ai_result['ai_probability']:.1%}). \"\n",
    "                   f\"Recommend discussing with student about AI usage policies.\")\n",
    "        \n",
    "        elif plag_result['is_plagiarized']:\n",
    "            sources = plag_result.get('sources_found', [])\n",
    "            if sources:\n",
    "                source_str = ', '.join(sources)\n",
    "                return f\"PLAGIARISM DETECTED: Code matches known sources ({source_str})\"\n",
    "            elif plag_result.get('submission_matches'):\n",
    "                return f\"PLAGIARISM DETECTED: Code matches other student submissions\"\n",
    "            else:\n",
    "                return \"PLAGIARISM DETECTED: Code matches existing patterns\"\n",
    "        \n",
    "        elif ai_result['ai_probability'] > 0.4:\n",
    "            return \"MODERATE RISK: Some AI-like patterns detected. May warrant review.\"\n",
    "        \n",
    "        elif plag_result['risk_level'] == 'LOW':\n",
    "            return \"ACCEPTABLE: Code uses common algorithm patterns (normal)\"\n",
    "        \n",
    "        else:\n",
    "            return \"CLEAN: No significant issues detected\"\n",
    "    \n",
    "    def _determine_action(self, ai_result: Dict, plag_result: Dict) -> str:\n",
    "        \"\"\"Determine recommended action\"\"\"\n",
    "        \n",
    "        if ai_result['is_ai_generated'] and plag_result['is_plagiarized']:\n",
    "            return \"BLOCK_AND_REPORT\"\n",
    "        elif plag_result['is_plagiarized'] and plag_result['risk_level'] == 'HIGH':\n",
    "            return \"INVESTIGATE\"\n",
    "        elif ai_result['is_ai_generated'] and ai_result['risk_level'] == 'HIGH':\n",
    "            return \"FLAG_FOR_REVIEW\"\n",
    "        elif ai_result['ai_probability'] > 0.4 or plag_result['risk_level'] != 'NONE':\n",
    "            return \"MONITOR\"\n",
    "        else:\n",
    "            return \"ACCEPT\"\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all LRU caches\"\"\"\n",
    "        if hasattr(self.plagiarism_detector, '_cached_normalize'):\n",
    "            self.plagiarism_detector._cached_normalize.cache_clear()\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    checker = OptimizedCodeChecker()\n",
    "    \n",
    "    test_code = '''\n",
    "def bubble_sort(arr):\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr) - i - 1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "    return arr\n",
    "    '''\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE CODE ANALYSIS TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = checker.analyze_code(test_code)\n",
    "    \n",
    "    print(f\"\\nOVERALL ASSESSMENT:\")\n",
    "    print(f\"  Suspicious: {result['overall_suspicious']}\")\n",
    "    print(f\"  Risk Level: {result['overall_risk_level']}\")\n",
    "    print(f\"  Action: {result['action']}\")\n",
    "    print(f\"  Recommendation: {result['recommendation']}\")\n",
    "    \n",
    "    print(f\"\\nAI DETECTION:\")\n",
    "    print(f\"  Probability: {result['ai_detection']['ai_probability']:.1%}\")\n",
    "    print(f\"  Risk: {result['ai_detection']['risk_level']}\")\n",
    "    \n",
    "    print(f\"\\nPLAGIARISM DETECTION:\")\n",
    "    print(f\"  Is Plagiarized: {result['plagiarism_detection']['is_plagiarized']}\")\n",
    "    print(f\"  Confidence: {result['plagiarism_detection']['confidence']:.1%}\")\n",
    "    print(f\"  Risk: {result['plagiarism_detection']['risk_level']}\")\n",
    "    if result['plagiarism_detection']['pattern_matches']:\n",
    "        print(f\"  Pattern Matches: {len(result['plagiarism_detection']['pattern_matches'])}\")\n",
    "    \n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85031b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
